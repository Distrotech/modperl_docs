=head1 NAME

Developing and Running Tests with the C<Apache::Test> Framework

=head1 Introduction

This chapter is talking about the C<Apache::Test> framework, and in
particular explains how to:

=over

=item * run existing tests

=item * setup a testing environment

=item * develop new tests

=back

But first let's introduce the C<Apache::Test> framework.

The C<Apache::Test> framework is designed for easy writing of tests
that has to be run under Apache webserver (not necessarily
mod_perl). Originally designed for the mod_perl Apache module, it was
extended to be used for any Apache module.

The tests themselves are written in Perl, and the framework will provide an
extensive functionality which makes the tests writing a simple and
therefore enjoyable process.

If you have ever written or looked at the tests most Perl modules come
with, C<Apache::Test> uses the same concept. The script I<t/TEST> is
running all the files ending with I<.t> it finds in the I<t/>
directory. When executed a typical test prints the following:

  1..3     # going to run 3 tests
  ok 1     # the first  test has passed
  ok 2     # the second test has passed
  not ok 3 # the third  test has failed

Every C<ok> or C<not ok> is followed by the number which tells which
sub-test has succeeded or failed.

I<t/TEST> uses the C<Test::Harness> module which intercepts the
C<STDOUT> stream, parses it and at the end of the tests print the
results of the tests running: how many tests and sub-tests were run,
how many succeeded, skipped or failed.

Some tests may be skipped by printing:

  1..0 # all tests in this file are going to be skipped.

Usually a test may be skipped when some feature is optional and/or
prerequisites are not installed on the system, but this is not
critical for the usefulness of the test. Once you test that you cannot
proceed with the tests and it's not a must pass test, you just skip
it.

It's important to know that there is a special verbose mode, enabled
with the I<-v> option, in which everything printed by the test goes to
C<STDOUT>. So for example if your test does this:

  print "# testing : feature foo\n";
  print "# expected: $expected\n";
  print "# received: $received\n";
  ok $expected eq $received;

in the normal mode, you won't see any of these prints. But if you run
the test with I<t/TEST -v>, you will see something like this:

  # testing : feature foo
  # expected: 2
  # received: 2
  ok 2

When you develop the test you should always put the debug statements
there, and once the test works for you do not comment out or delete
these debug statements. This is because if some user reports a failure
in some test, you can ask him to run the failing test in the verbose
mode and send you back the report. It'll be much easier to understand
what the problem is if you get these debug printings from the user.

In the section L<"How to Write Tests"> we discuss a few helper
functions which make the tests writing easier.

For more details about the C<Test::Harness> module please refer to its
manpage. Also see the C<Test> manpage about Perl's test suite.

=head1 Prerequisites

In order to use C<Apache::Test> it has to be installed first.

Install C<Apache::Test> using the familiar procedure:

  % cd Apache-Test
  % perl Makefile.PL
  % make && make test && make install

If you install mod_perl 2.x, you get C<Apache::Test> installed as
well.

=head1 How to Run Tests

It's much easier to copy-cat things, than creating from scratch.  It's
much easier to develop tests, when you have some existing system that
you can test, see how it works and build your own testing environment
in a similar fashion. Therefore let's first look at how the existing
test enviroments work.

You can look at the modperl-2.0's or httpd-test's (I<perl-framework>)
testing environments which both use C<Apache::Test> for their test
suites.

Running tests is just like for any CPAN Perl module; first we create
the I<Makefile> and build everything with I<make>:

  % perl Makefile.PL [options]
  % make

Now we can do the testing. You can run the tests in two ways. The
first one is usual:

  % make test

but it adds quite an overhead, since it has to check that everything
is up to date (the usual C<make> source change control). Therefore
faster to run the tests directly via:

  % t/TEST

In case something goes wrong you should run the tests in the verbose
mode:

  % t/TEST -v

In this case the test may print useful information, like what values
it expects and what values it receives, given that the test is written
to report these. In the silent mode (without C<-v>) these printouts
are suppressed by the test suite.

When debugging problems it helps to keep the I<error_log> file open in
another console, and see the debug output in the real time via
tail(1):

  % tail -f t/logs/error_log

Of course this file gets created only when the server starts, so you
cannot run tail(1) on it before the server starts.

[F] Later on we will talk about I<-clean> option, for now just
remember that if you use it I<t/logs/error_log> is deleted, therefore
you have to run the tail(1) command again, when the server is
started. [/F]

If you have to run the same tests repeatedly, in most cases you don't
want to wait for the server to start every time. You can start it
once:

  % t/TEST -start

and then run tests without restarting the server using I<-run> option:

  % t/TEST -run

Once the server is started you can modify I<.t> files and rerun the
tests without restarting the server. However if you modify response
handlers, you must restart the server for changes to take an
effect. If C<Apache::Reload> is used and configured to automatically
reload the handlers when they change you don't have to restart the
server. For example to automatically reload all C<TestDirective::*>
modules when they change on the disk, add to I<t/conf/extra.conf.in>:

  PerlModule Apache::Reload
  PerlInitHandler Apache::Reload
  PerlSetVar ReloadAll Off
  PerlSetVar ReloadModules "TestDirective::*"

and restart the server.

If none of the tests are specified at the command line all tests found
in the I<t> directory (files ending with I<.t> are recongnized as
tests) will be run. To run specific tests, they should be explicitly
specified. For example to run the test file I<t/protocol/echo.t> we
execute:

  % t/TEST -run protocol/echo

notice that you don't have to add the I<t/> prefix and I<.t> extension
for the test filenames if you specify them explicitly.

When you run specific tests you may want to run them in the verbose
mode, and depending on how the test was written, you may get more
debug information under this mode. This mode is turned on with I<-v>
option:

  % t/TEST -run -v protocol/echo

You can run all the tests in a single directory by just passing this
directory as an argument. You can pass more than one test or directory
name at the same time. Thefore assuming that the directory
I<t/protocol> includes only two files: I<echo.t> and I<eliza.t>--the
following two commands are equivalent:

  % t/TEST -run protocol/echo protocol/eliza
  % t/TEST -run protocol

The command:

  % t/TEST -start

always stops the server first if any is running. In case you have a
server runnning on the same port, (for example if you develop the a
few tests at the same time in different trees), you should run the
server on a different port. C<Apache::Test> will try to automatically
pick a free port, but you can explicitly tell on which port to run,
using the I<-port> configuration option:

  % t/TEST -start -port 8799

or by setting an evironment variable C<APACHE_PORT> to the desired
value before starting the server.

META: a lot more stuff to go here from the pods/modperl_dev.pod and
Apache-Test/README

=head1 How to Setup Testing Environment

We will assume that you setup your testing environment even before you
have started developing the module, which is a very smart thing to do.
Of course it'll take you more time upfront, but it'll will save you a
lot of time as you develop and debug your code. The L<extreme
programming methodology|/item_extreme_programming_methodology> says
that tests should be written before starting the code development.

So the first thing is to create a package and all the helper files, so
later on we can distribute it on CPAN. We are going to develop an
C<Apache::Amazing> module as an example.

  % h2xs -AXn Apache::Amazing
  Writing Apache/Amazing/Amazing.pm
  Writing Apache/Amazing/Makefile.PL
  Writing Apache/Amazing/README
  Writing Apache/Amazing/test.pl
  Writing Apache/Amazing/Changes
  Writing Apache/Amazing/MANIFEST

C<h2xs> is a nifty utility that gets installed together with Perl and
helps us to create some of the files we will need later.

However we are going to use a little bit different files layout,
therefore we are going to move things around a bit.

We want our module to live in the I<Apache-Amazing> directory, so we
do:

  % mv Apache/Amazing Apache-Amazing
  % rmdir Apache

From now on the I<Apache-Amazing> directory is our working directory.

  % cd Apache-Amazing

We don't need the I<test.pl>. as we are going to create a whole
testing environment:

  % rm test.pl

We want our package to reside under the I<lib> directory:

  % mkdir lib
  % mkdir lib/Apache
  % mv Amazing.pm lib/Apache

Now we adjust the I<lib/Apache/Amazing.pm> to look like this:

  file:lib/Apache/Amazing.pm
  --------------------------
  package Apache::Amazing;
  
  use strict;
  use warnings;
  
  use Apache::RequestRec ();
  use Apache::RequestIO ();
  
  $Apache::Amazing::VERSION = '0.01';
  
  use Apache::Const -compile => 'OK';
  
  sub handler {
      my $r = shift;
      $r->content_type('text/plain');
      $r->print("Amazing!");
      return Apache::OK;
  }
  1;
  __END__
  ... pod documentation goes here...

The only thing it does is setting the I<text/plain> header and
responding with I<"Amazing!">.

Next adjust or create the I<Makefile.PL> file:

  file:Makefile.PL
  ----------------
  require 5.6.1;
  
  use ExtUtils::MakeMaker;
  
  use lib qw(../blib/lib lib );
  
  use Apache::TestMM qw(test clean); #enable 'make test'
  
  # prerequisites
  my %require =
    (
     "Apache::Test" => "", # any version will do
    );

  # accept the configs from comman line
  Apache::TestMM::filter_args();
  Apache::TestMM::generate_script('t/TEST');

  WriteMakefile(
      NAME         => 'Apache::Amazing',
      VERSION_FROM => 'lib/Apache/Amazing.pm',
      PREREQ_PM    => \%require,
      clean        => {
                       FILES => "@{ clean_files() }",
                      },
      ($] >= 5.005 ?
          (ABSTRACT_FROM => 'lib/Apache/Amazing.pm',
           AUTHOR        => 'Stas Bekman <stas (at) stason.org>',
          ) : ()
      ),
  );
  
  sub clean_files {
      return [@scripts];
  }

C<Apache::TestMM> will do a lot of thing for us, such as building a
complete Makefile with proper I<'test'> and I<'clean'> targets,
automatically converting I<.PL> and I<conf/*.in> files and more.

As you see we specify a prerequisites hash with I<Apache::Test> in it,
so if the package gets distributed on CPAN, C<CPAN.pm> shell will know
to fetch and install this required package.

Next we create the test suite, which will reside in the I<t>
directory:

  % mkdir t

First we create I<t/TEST.PL> which will be automatically converted
into I<t/TEST> during I<perl Makefile.PL> stage:

  file:t/TEST.PL
  --------------
  #!perl
  
  use strict;
  use warnings FATAL => 'all';
  
  use lib qw(lib);
  
  use Apache::TestRunPerl ();
  
  Apache::TestRunPerl->new->run(@ARGV);

Assuming that C<Apache::Test> is already installed on your system and
Perl can find it. If not you should tell Perl where to find it. For
example you could add:

  use lib qw(../Apache-Test/lib);

to I<t/TEST.PL>, if C<Apache::Test> is located in a parallel
directory.

As you can see we didn't write the real path to the Perl executable,
but C<#!perl>. When I<t/TEST> is created the correct path will be
placed there automatically.

Next we need to prepare extra Apache configuration bits, which will
reside in I<t/conf>:

  % mkdir t/conf

We create the I<t/conf/extra.conf.in> file which will be automatically
converted into I<t/conf/extra.conf> before the server starts. If the
file has any placeholders like C<@documentroot@>, these will be
replaced with the real values specific for the used server. In our
case we put the following configuration bits into this file:

  file:t/conf/extra.conf.in
  -------------------------
  # this file will be Include-d by @ServerRoot@/httpd.conf
  
  # where Apache::Amazing can be found
  PerlSwitches -Mlib=@ServerRoot@/../lib
  # preload the module
  PerlModule Apache::Amazing
  <Location /test/amazing>
      SetHandler modperl
      PerlResponseHandler Apache::Amazing
  </Location>

As you can see we just add a simple E<lt>LocationE<gt> container and
tell Apache that the namespace I</test/amazing> should be handled by
C<Apache::Amazing> module running as a mod_perl handler.

As mentioned before you can use C<Apache::Reload> to automatically
reload the modules under development when they change. The setup for
this module goes into I<t/conf/extra.conf.in> as well.

  file:t/conf/extra.conf.in
  -------------------------
  PerlModule Apache::Reload
  PerlPostReadRequestHandler Apache::Reload
  PerlSetVar ReloadAll Off
  PerlSetVar ReloadModules "Apache::Amazing"

For more information about C<Apache::Reload> refer to its manpage.

Now we can create a simple test:

  file:t/basic.t
  -----------
  use strict;
  use warnings FATAL => 'all';
  
  use Apache::Amazing;
  use Apache::Test;
  use Apache::TestUtil;
  
  plan tests => 2;
  
  ok 1; # simple load test
  
  my $config = Apache::Test::config();
  my $url = '/test/amazing';
  my $data = $config->http_raw_get($url);
  
  ok t_cmp(
           "Amazing!",
           $data,
           "basic test",
          );

Now create the README file.

  % touch README

Don't forget to put in the relevant information about your module, or
arrange for C<ExtUtils::MakeMaker::WriteMakefile()> to do this for you
with:

  file:Makefile.PL
  ----------------
  WriteMakefile(
               ...
      dist  => {
                PREOP => 'pod2text lib/Apache/Amazing.pm > $(DISTVNAME)/README',
               },
               ...
               );

in this case C<README> will be created from the documenation POD
sections in I<lib/Apache/Amazing.pm>, but the file has to exists for
I<make dist> to succeed.

and finally we adjust or create the C<MANIFEST> file, so we can
prepare a complete distribution. Therefore we list all the files that
should enter the distribution including the C<MANIFEST> file itself:

  file:MANIFEST
  -------------
  lib/Apache/Amazing.pm
  t/TEST.PL
  t/basic.t
  t/conf/extra.conf.in
  Makefile.PL
  Changes
  README
  MANIFEST

That's it. Now we can build the package. But we need to know where
C<apxs> utility from the installed on our system Apache is located. We
pass its path as an option:

  % perl Makefile.PL -apxs ~/httpd/prefork/bin/apxs
  % make
  % make test

  basic...........ok
  All tests successful.
  Files=1, Tests=2,  1 wallclock secs ( 0.52 cusr +  0.02 csys =  0.54 CPU)

To install the package run:

  % make install

Now we are ready to distribute the package on CPAN:

  % make dist

will create the package which can be immediately uploaded to CPAN. In
this example the generated source package with all the required files
will be called: I<Apache-Amazing-0.01.tar.gz>.

The only thing that we haven't done and hope that you will do is to
write the POD sections for the C<Apache::Amazing> module, explaining
how amazingly it works and how amazingly it can be deployed by other
users.

=head1 Apache::Test Framework's Architecture

In the previous section we have written a basic test, which doesn't do
much. In the following sections we will explain how to write more
elaborate tests.

When you write the test for Apache, unless you want to test some
static resource, like fetching a file, usually you have to write a
response handler and the corresponding test that will generate a
request which will exercise this response handler and verify that the
response is as expected. From now we may call these two parts as
client and server parts of the test, or request and response parts of
the test.

In some cases the response part of the test runs the test inside
itself, so all it requires from the request part of the test, is to
generate the request and print out a complete response without doing
anything else. In such cases C<Apache::Test> can auto-generate the
client part of the test for you.

=head2 Developing Response-only Part of a Test

If you write only a response part of the test, C<Apache::Test> will
automatically generate the corresponding test part that will generated
the response. In this case your test should print I<'ok 1'>, I<'not ok
2'> responses as usual tests do. The autogenerated request part will
receive the response and print them out automatically completing the
C<Test::Harness> expectations.

The corresponding request part of the test is named just like the
response part, using the following translation:

  $response_test =~ s|t/[^/]+/Test([^/]+)/(.*).pm$|t/\L$1\E/$2.t|;

so for example I<t/response/TestApache/write.pm> becomes:
I<t/apache/write.t>.

If we look at the autogenerated test I<t/apache/write.t>, we can see
that it start with the warning that it has been autogenerated, so you
won't attempt to change it, following by the trace of the calls that
generated this test, in case you want to trace back to who generated
the test, and finally it loads the C<Apache::TestConfig> module and
prints a raw response from the the response part:

  use Apache::TestConfig ();
  print Apache::TestConfig->thaw->http_raw_get("/TestApache::write");

As you can see the request URI is autogenerated from the response test
name:

  $response_test =~ s|.*/([^/]+)/(.*).pm$|/$1::$2|;

So I<t/response/TestApache/write.pm> becomes: I</TestApache::write>.

Now a simple response test may look like this:

  package TestApache::write;
  
  use strict;
  use warnings FATAL => 'all';
  
  use constant BUFSIZ => 512; #small for testing
  use Apache::Const -compile => 'OK';
  
  sub handler {
      my $r = shift;
      $r->content_type('text/plain');
  
      $r->write("1..2\n");
      $r->write("ok 1")
      $r->write("not ok 2")
  
      Apache::OK;
  }
  1;

[F] C<Apache::Const> is mod_perl 2.x's package, if you test under 1.x,
use the C<Apache::Constants> module instead [/F].

The configuration part for this test will be autogenerated by the
C<Apache::Test> framework and added to the autogenerated file
I<t/conf/httpd.conf>. In our case the following configuration section
will be added.

  <Location /TestApache::write>
     SetHandler modperl
     PerlResponseHandler TestApache::write
  </Location>

You should remember to run:

  % t/TEST -clean

so when you run your new tests the new configuration will be added.

=head2 Developing Response and Request Parts of a Test

But in most cases you want to write a two parts test where the client
(request) parts generates various requests and tests the responses.

It's possible that the client part tests a static file or some other
feature that doesn't require a dynamic response. In this case, only
the request part of the test should be written.

If you need to write the complete test, with two parts, you proceed
just like in the previous section, but now you write the client part
of the test by yourself. It's quite easy, all you have to do is to
generate requests and check the response. So a typical test will look
like this:

  t/apache/cool.t
  -----------
  use strict;
  use warnings FATAL => 'all';

  use Apache::Test;
  use Apache::TestUtil;
  use Apache::TestRequest;

  plan tests => 1; # plan one test.

  Apache::TestRequest::module('default');

  my $config   = Apache::Test::config();
  my $hostport = Apache::TestRequest::hostport($config) || '';
  t_debug("connecting to $hostport");

  my $received = $config->http_raw_get("/TestApache::cool", undef);
  my $expected = "COOL";

  ok t_cmp(
           $expected,
           $received,
           "testing TestApache::cool",
            );

See the L<Apache::TestUtil> manpage for more info on the t_cmp()
function (e.g. it works with regexs as well).

And the corresponding response part:

  t/response/TestApache/cool.pm:
  --------------------------
  package TestApache::cool;
  
  use strict;
  use warnings FATAL => 'all';
  
  use Apache::Const -compile => 'OK';
  
  sub handler {
      my $r = shift;
      $r->content_type('text/plain');
  
      $r->write("COOL");
  
      Apache::OK;
  }
  1;

Again, remember to run I<t/TEST -clean> before running the new test so
the configuration will be created for it.

As you can see the test generates a request to I</TestApache::cool>,
and expects it to return I<"COOL">. If we run the test:

  % ./t/TEST t/apache/cool

We see:

  apache/cool....ok
  All tests successful.
  Files=1, Tests=1,  1 wallclock secs ( 0.52 cusr +  0.02 csys =  0.54 CPU)

But if we run it in the debug (verbose) mode, we can actually see what
we are testing, what was expected and what was received:

  apache/cool....1..1
  # connecting to localhost:8529
  # testing : testing TestApache::cool
  # expected: COOL
  # received: COOL
  ok 1
  ok
  All tests successful.
  Files=1, Tests=1,  1 wallclock secs ( 0.49 cusr +  0.03 csys =  0.52 CPU)

So in case in our simple test we have received something different
from I<COOL> or nothing at all, we can immediately see what's the
problem.

The name of the request part of the test is very important. If
C<Apache::Test> cannot find the corresponding test for the response
part it'll automatically generate one and in this case it's probably
not what you want. Therefore when you choose the filename for the
test, make sure to pick the same C<Apache::Test> will pick. So if the
response part is named: I<t/response/TestApache/cool.pm> the request
part should be named I<t/apache/cool.t>. See the regular expression
that does that in the previous section.

=head2 Developing Test Response Handlers in C

If you need to exercise some C API and you don't have a Perl glue for
it, you can still use C<Apache::Test> for the testing. It allows you
to write response handlers in C and makes it easy to integrate these
with other Perl tests and use Perl for request part which will
exercise the C module.

The C modules look just like standard Apache C modules, with a couple
of differences to:

=over

=item a

help them fit into the test suite

=item b

allow them to compile nicely with Apache 1.x or 2.x.

=back

The I<httpd-test> ASF project is a good example to look at. The C
modules are located under: I<httpd-test/perl-framework/c-modules/>.
Look at I<c-modules/echo_post/echo_post.c> for a nice simple example.
C<mod_echo_post> simply echos data that is C<POST>ed to it.

The differences between vairous tests may be summarized as follows:

=over

=item *

If the first line is:

  #define HTTPD_TEST_REQUIRE_APACHE 1

or

  #define HTTPD_TEST_REQUIRE_APACHE 2

then the test will be skipped unless the version matches. If a module
is compatible with the version of Apache used then it will be
automatically compiled by I<t/TEST> with C<-DAPACHE1> or C<-DAPACHE2>
so you can conditionally compile it to suit different httpd versions.

=item *

If there is a section bounded by:

  #if CONFIG_FOR_HTTPD_TEST
  ...
  #endif

in the I<.c> file then that section will be inserted verbatim into
I<t/conf/httpd.conf> by I<t/TEST>.

=back

There is a certain amount of magic which hopefully allows most modules
to be compiled for Apache 1.3 or Apache 2.0 without any conditional
stuff.  Replace XXX with the module name, for example echo_post or
random_chunk:

=over

=item *

You should:

  #include "apache_httpd_test.h" 

which should be preceded by an:

  #define APACHE_HTTPD_TEST_HANDLER XXX_handler

I<apache_httpd_test.h> pulls in a lot of required includes and defines
some constants and types that are not defined for Apache 1.3.

=item *

The handler function should be:

  static int XXX_handler(request_rec *r);

=item *

At the end of the file should be an:

  APACHE_HTTPD_TEST_MODULE(XXX)

where XXX is the same as that in C<APACHE_HTTPD_TEST_HANDLER>. This
will generate the hooks and stuff.

=back


=head1 How to Write Tests

All the communications between tests and C<Test::Harness> which
executes them is done via STDOUT. I.e. whatever tests want to report
they do by printing something to STDOUT. If a test wants to print some
debug comment it should do it starting on a separate line, and each
debug line should start with C<#>. The t_debug() function from the
C<Apache::TestUtil> package should be used for that purpose.



=head2 Defining How Many Sub-Tests Are to Be Run

Before sub-tests of a certain test can be run it has to declare how
many sub-tests it is going to run. In some cases the test may decide
to skip some of its sub-tests or not to run any at all. Therefore the
first thing the test has to print is:

  1..M\n

where M is a positive integer. So if the test plans to run 5 sub-tests
it should do:

  print "1..5\n";

In C<Apache::Test> this is done as follows:

  use Apache::Test;
  plan tests => 5;



=head2 Skipping a Whole Test

Sometimes when the test cannot be run, because certain prerequisites
are missing. To tell C<Test::Harness> that the whole test is to be
skipped do:

  print "1..0 # skipped because of foo is missing\n";

The optional comment after C<# skipped> will be used as a reason for
test's skipping. Under C<Apache::Test> the optional last argument to
the plan() function can be used to define prerequisites and skip the
test:

  use Apache::Test;
  plan tests => 5, $test_skipping_prerequisites;

This last argument can be:

=over

=item * a C<SCALAR>

the test is skipped if the scalar has a false value. For example:

  plan tests => 5, 0;

=item * an C<ARRAY> reference

have_module() is called for each value in this array. The test is
skipped if have_module() returns false (which happens when at least
one C or Perl module from the list cannot be found). For example:

  plan tests => 5, [qw(mod_index mod_mime)];

=item * a C<CODE> reference

the tests will be skipped if the function returns a false value. For
example:

    plan tests => 5, \&have_lwp;

the test will be skipped if LWP is not available

=back

There is a number of useful functions whose return value can be used
as a last argument for plan():

=over

=item * skip_unless()

Instead of using a scalar as a last argument to plan() to tell whether
to skip the test or not, it's better to use skip_unless() which also
prints the reason for skipping the test if the condition is not
satisfied. For example:

  plan tests => 5, skip_unless(sub { $a == $b }, "$a != $b");

skip_unless() executes the code reference in the first argument and if
it returns a false value C<$reason> gets printed as a reason for test
skipping.

=item * have_module()

have_module() tests for existance of Perl modules or C modules
I<mod_*>. It accepts a list of modules or a reference to the list.  If
at least one of the modules is not found it returns a false value,
otherwise it returns a true value. For example:

  plan tests => 5, have_module qw(Chatbot::Eliza Apache::AI);

will skip the whole test if both Perl modules C<Chatbot::Eliza> and
C<Apache::AI> are not available.

=item * have_perl()

have_perl('foo') checks whether the value of C<$Config{foo}> or
C<$Config{usefoo}> is equal to I<'define'>. For example:

  plan tests => 2, have_perl 'ithreads';

if Perl wasn't compiled with C<-Duseithreads> the condition will be
false and the test will be skipped.

=item * have_lwp()

Tests whether the Perl module LWP is installed.

=item * have_http11()

Tries to tell LWP that sub-tests need to be run under HTTP 1.1
protocol. Fails if the installed version of LWP is not capable of
doing that.

=item * have_cgi()

tests whether mod_cgi or mod_cgid is available.

=item * have_apache()

tests for a specific version of httpd. For example:

  plan tests => 2, have_apache 2;

will skip the test if not run under httpd 2.x.

=back


=head2 Skipping Numerous Tests

Just like you can tell C<Apache::Test> to run only specific tests, you
can tell it to run all but a few tests.

If all files in a directory I<t/foo> should be skipped, create:

  t/foo/all.t:
  ------------
  print "1..0\n";

Alternatively you can specify which tests should be skipped from a
single file I<t/SKIP>. This file includes a list of tests to be
skipped. You can include comments starting with C<#> and you can use
the C<*> wildcharacter for multiply files matching.

For example if in mod_perl 2.0 test suite we create the following file:

  t/SKIP:
  -------
  # skip all files in protocol
  protocol
  
  # skip basic cgi test
  modules/cgi.t
  
  # skip all filter/input_* files
  filter/input*.t

In our example the first pattern specifies the directory name
I<protocol>, since we want to skip all tests in it. But since the
skipping is done based on matching the skip patterns from t/SKIP
against a list of potential tests to be run, some other tests may be
skipped as well if they match the pattern. Therefore it's safer to use
a pattern like this:

  protocol/*.t

The second pattern skips a single test I<modules/cgi.t>. Note that you
shouldn't specify the leading I<t/>. The I<.t> extension is optional,
so you can tell:

  # skip basic cgi test
  modules/cgi

The last pattern tells C<Apache::Test> to skip all the tests starting
with I<filter/input>.

=head2 Reporting a Success or a Failure of Sub-tests

After printing the number of planned sub-tests, and assuming that the
test is not skipped, the tests is running its sub-tests and each
sub-test is expected to report its success or failure by printing
I<ok> or I<not ok> respectively followed by its sequential number and
a new line. For example:

  print "ok 1\n";
  print "not ok 2\n";
  print "ok 3\n";

In C<Apache::Test> this is done using the ok() function which prints
I<ok> if its argument is a true value, otherwise it prints I<not
ok>. In addition it keeps track of how many times it was called, and
every time it prints an incremental number, therefore you can move
sub-tests around without needing to remember to adjust sub-test's
sequential number, since now you don't need them at all. For example
this test snippet:

  use Apache::Test;
  use Apache::TestUtil;
  plan tests => 3;
  ok "success";
  t_debug("expecting to fail next test");
  ok "";
  ok 0;

will print:

  1..3
  ok 1
  # expecting to fail next test
  not ok 2
  not ok 3

Most of the sub-tests perform one of the following things:

=over

=item *

test whether some variable is defined:

  ok defined $object;

=item *

test whether some variable is a true value:

  ok $value;

or a false value:

  ok !$value;

=item *

test whether a received from somewhere value is equal to an expected
value:

  $expected = "a good value";
  $received = get_value();
  ok defined $received && $received eq $expected;

=back






=head2 Skipping Sub-tests

If the standard output line contains the substring I< # Skip> (with
variations in spacing and case) after I<ok> or I<ok NUMBER>, it is
counted as a skipped test. C<Test::Harness> reports the text after I<
# Skip\S*\s+> as a reason for skipping. So you can count a sub-test as 
a skipped as follows:

  print "ok 3 # Skip for some reason\n";

or using the C<Apache::Test>'s skip() function which works similarly
to ok():

  skip $should_skip, $test_me;

so if C<$should_skip> is true, the test will be reported as
skipped. The second argument is the one that's sent to ok(), so if
C<$should_skip> is true, a normal ok() sub-test is run. The following
example represent four possible outcomes of using the skip() function:

  skip_subtest_1.t
  --------------
  use Apache::Test;
  plan tests => 4;
  
  my $ok     = 1;
  my $not_ok = 0;
  
  my $should_skip = "foo is missing";
  skip $should_skip, $ok;
  skip $should_skip, $not_ok;
  
  $should_skip = '';
  skip $should_skip, $ok;
  skip $should_skip, $not_ok;

now we run the test:

  % ./t/TEST -run -v skip_subtest_1
  skip_subtest_1....1..4
  ok 1 # skip foo is missing
  ok 2 # skip foo is missing
  ok 3
  not ok 4
  # Failed test 4 in skip_subtest_1.t at line 13
  Failed 1/1 test scripts, 0.00% okay. 1/4 subtests failed, 75.00% okay.

As you can see since C<$should_skip> had a true value, the first two
sub-tests were explicitly skipped (using C<$should_skip> as a reason),
so the second argument to skip didn't matter. In the last two
sub-tests C<$should_skip> had a false value therefore the second
argument was passed to the ok() function. Basically the following
code:

  $should_skip = '';
  skip $should_skip, $ok;
  skip $should_skip, $not_ok;

is equivalent to:

  ok $ok;
  ok $not_ok;

C<Apache::Test> also allows to write tests in such a way that only
selected sub-tests will be run.  The test simply needs to switch from
using ok() to sok().  Where the argument to sok() is a CODE reference
or a BLOCK whose return value will be passed to ok().  If sub-tests
are specified on the command line only those will be run/passed to
ok(), the rest will be skipped.  If no sub-tests are specified, sok()
works just like ok().  For example, you can write this test:

  skip_subtest_2.t
  --------------
  use Apache::Test;
  plan tests => 4;
  sok {1};
  sok {0};
  sok sub {'true'};
  sok sub {''};

and then ask to run only sub-tests 1 and 3 and to skip the rest.

  % ./t/TEST -v skip_subtest_2 1 3
  skip_subtest_2....1..4
  ok 1
  ok 2 # skip skipping this subtest
  ok 3
  ok 4 # skip skipping this subtest
  ok, 2/4 skipped:  skipping this subtest
  All tests successful, 2 subtests skipped.

Only the sub-tests 1 and 3 get executed.

A range of sub-tests to run can be given using the Perl's range
operand:

  % ./t/TEST -v skip_subtest_2 2..4
  skip_subtest_2....1..4
  ok 1 # skip askipping this subtest
  not ok 2
  # Failed test 2
  ok 3
  not ok 4
  # Failed test 4
  Failed 1/1 test scripts, 0.00% okay. 2/4 subtests failed, 50.00% okay.

In this run, only the first sub-test gets executed.

=head2 Todo Sub-tests

In a safe fashion to skipping specific sub-tests, it's possible to
declare some sub-tests as I<todo>. This distinction is useful when we
know that some sub-test is failing but for some reason we want to flag
it as a todo sub-test and not as a broken test. C<Test::Harness>
recognizes I<todo> sub-tests if the standard output line contains the
substring I< # TODO> after I<not ok> or I<not ok NUMBER> and is
counted as a todo sub-test.  The text afterwards is the explanation of
the thing that has to be done before this sub-test will succeed. For
example:

  print "not ok 42 # TODO not implemented\n";

In C<Apache::Test> this can be done with passing a reference to a list
of sub-tests numbers that should be marked as I<todo> sub-test:

  plan tests => 7, todo => [3, 6];

In this example sub-tests 3 and 6 will be marked as I<todo> sub-tests.





=head2 Making it Easy to Debug

Ideally we want all the tests to pass, reporting minimum noise or none
at all. But when some sub-tests fail we want to know the reason for
their failure. If you are a developer you can dive into the code and
easily find out what's the problem, but when you have a user who has a
problem with the test suite it'll make his and your life much easier
if you make it easy for the user to report you the exact problem.

Usually this is done by printing the comment of what the sub-test
does, what is the expected value and what's the received value. This
is a good example of debug friendly sub-test:

  debug_comments.t
  ----------------
  use Apache::Test;
  use Apache::TestUtil;
  plan tests => 1;
  
  t_debug("testing feature foo");
  $expected = "a good value";
  $received = "a bad value";
  t_debug("expected: $expected");
  t_debug("received: $received");
  ok defined $received && $received eq $expected;

If in this example C<$received> gets assigned I<a bad value> string,
the test will print the following:

  % t/TEST debug_comments
  debug_comments....FAILED test 1

No debug help here, since in a non-verbose mode the debug comments
aren't printed.  If we run the same test using the verbose mode,
enabled with C<-v>:

  % t/TEST -v debug_comments
  debug_comments....1..1
  # testing feature foo
  # expected: a good value
  # received: a bad value
  not ok 1

we can see exactly what's the problem, by visual expecting of the
expected and received values.

It's true that adding a few print statements for each sub tests is
cumbersome, and adds a lot of noise, when you could just tell:

  ok "a good value" eq "a bad value";

but no fear, C<Apache::TestUtil> comes to help. The function t_cmp()
does all the work for you:

  use Apache::Test;
  use Apache::TestUtil;
  ok t_cmp(
      "a good value",
      "a bad value",
      "testing feature foo");

In addition it will handle undef'ined values as well, so you can do:

  ok t_cmp(undef, $expected, "should be undef");





=head2 Tie-ing STDOUT to a Response Handler Object

It's possible to run the sub-tests in the response handler, and simply
return them as a response to the client which in turn will print them
out. Unfortunately in this case you cannot use ok() and other
functions, since they print and don't return the results, therefore
you have to do it manually. For example:

  sub handler {
      my $r = shift;
  
      $r->print("1..2\n");
      $r->print("ok 1\n");
      $r->print("not ok 2\n");
    
      return Apache::OK;
  }

now the client should print the response to STDOUT for
C<Test::Harness> processing.

If the response handler is configured as:

  SetHandler perl-script

C<STDOUT> is already tied to the request object C<$r>. Therefore you
can now rewrite the handler as:

  use Apache::Test;
  sub handler {
      my $r = shift;
  
      Apache::Test::test_pm_refresh();
      plan tests => 2;
      ok "true";
      ok "";
    
      return Apache::OK;
  }

However to be on the safe side you also have to call
Apache::Test::test_pm_refresh() allowing plan() and friends to be
called more than once per-process.

Under different settings C<STDOUT> is not tied to the request object.
If the first argument to plan() is an object, such as an
C<Apache::RequestRec> object, C<STDOUT> will be tied to it. The
C<Test.pm> global state will also be refreshed by calling
C<Apache::Test::test_pm_refresh>. For example:

  use Apache::Test;
  sub handler {
      my $r = shift;
  
      plan $r, tests => 2;
      ok "true";
      ok "";
    
      return Apache::OK;
  }

Yet another alternative to handling the test framework printing inside
response handler is to use C<Apache::TestToString> class.

The C<Apache::TestToString> class is used to capture C<Test.pm> output
into a string.  Example:

  use Apache::Test;
  sub handler {
      my $r = shift;
  
      Apache::TestToString->start;
  
      plan tests => 2;
      ok "true";
      ok "";
    
      my $output = Apache::TestToString->finish;
      $r->print($output);
  
      return Apache::OK;
  }

In this example C<Apache::TestToString> intercepts and buffers all the
output from C<Test.pm> and can be retrieved with its finish()
method. Which then can be printed to the client in one
shot. Internally it calls Apache::Test::test_pm_refresh() to make sure
plan(), ok() and other functions() will work correctly more than one
test is running under the same interpreter.






=head2 Auto Configuration

If the test is comprised only from the request part, you have to
manually configure the targets you are going to use. This is usually
done in I<t/conf/extra.conf.in>.

If your tests are comprised from the request and response parts,
C<Apache::Test> automatically adds the configuration section for each
response handler it finds. For example for the response handler:

  package TestResponse::nice;
  ... some code
  1;

it will put into I<t/conf/httpd.conf>:

  <Location /TestResponse::nice>
      SetHandler modperl
      PerlResponseHandler TestResponse::nice
  </Location>

If you want to add some extra configuration directives, use the
C<__DATA__> section, as in this example:

  package TestResponse::nice;
  ... some code
  1;
  __DATA__
  PerlSetVar Foo Bar

These directives will be wrapped into the C<E<lt>LocationE<gt>>
section and placed into I<t/conf/httpd.conf>:

  <Location /TestResponse::nice>
      SetHandler modperl
      PerlResponseHandler TestResponse::nice
      PerlSetVar Foo Bar
  </Location>

This autoconfiguration feature was added to:

=over

=item *

simplify (less lines) test configuration.

=item *

ensure unique namespace for E<lt>Location ...E<gt>'s.

=item *

force E<lt>Location ...E<gt> names to be consistent.

=item *

prevent clashes within main configuration.

=back

If some directives are supposed to go to the base configuration,
i.e. not to be automatically wrapped into C<E<lt>LocationE<gt>> block,
you should use a special C<E<lt>BaseE<gt>>..C<E<lt>/BaseE<gt>> block:

  __DATA__
  <Base>
      PerlSetVar Config ServerConfig
  <Base>
  PerlSetVar Config LocalConfig

Now the autogenerated section will look like this:

  PerlSetVar Config ServerConfig
  <Location /TestResponse::nice>
     SetHandler modperl
     PerlResponseHandler TestResponse::nice
     PerlSetVar Config LocalConfig
  </Location>

As you can see the C<E<lt>BaseE<gt>>..C<E<lt>/BaseE<gt>> block has
gone. As you can imagine this block was added to support our virtue of
laziness, since most tests don't need to add directives to the base
configuration and we want to keep the configuration sections in tests
to a minimum and let Perl do the rest of the job for us.



META: Virtual host?

META: to be completed

=head2 Extending Startup Configuration for httpd and Perl

Sometimes you need to add extra I<httpd.conf> configuration and perl
startup specific to your project that uses C<Apache::Test>. This can
be accomplished by creating the desired files with an extension I<.in>
in the I<t/conf/> directory and running:

  panic% t/TEST -config

which for each file with the extension I<.in> will create a new file,
without this extension, convert any template placeholders into real
values and link it from the main I<httpd.conf>. The latter happens
only if the file have the following extensions:

=over

=item * .conf.in

will add to I<t/conf/httpd.conf>:

  Include foo.conf

=item * .pl.in

will add to I<t/conf/httpd.conf>:

  PerlRequire foo.pl

=item * other

other files with I<.in> extension will be processed as well, but not
linked from I<httpd.conf>.

=back

As mentioned before the converted files are created, any special token
in them are getting replaced with the appropriate values. For example
the token C<@ServerRoot@> will be replaced with the value defined by
the C<ServerRoot> directive, so you can write a file that does the
following:

  my-extra.conf.in
  ----------------
  PerlSwitches -Mlib=@ServerRoot@/../lib

and assuming that the I<ServerRoot> is I<~/modperl-2.0/t/>, when
I<my-extra.conf> will be created, it'll look like:

  my-extra.conf.in
  ----------------
  PerlSwitches -Mlib=~/modperl-2.0/t/../lib

The following tokens are valid:

META: complete

          'cgi_module_name' => 'mod_cgi',
          'conf_dir' => '/home/stas/httpd/prefork/conf',
          't_conf_file' => '/home/stas/apache.org/modperl-2.0/t/conf/httpd.conf',
          't_dir' => '/home/stas/apache.org/modperl-2.0/t',
          'cgi_module' => 'mod_cgi.c',
          'target' => 'httpd',
          'thread_module' => 'worker.c',
          'bindir' => '/home/stas/httpd/prefork/bin',
          'user' => 'stas',
          'top_dir' => '/home/stas/apache.org/modperl-2.0',
          'httpd_conf' => '/home/stas/httpd/prefork/conf/httpd.conf',
          'httpd' => '/home/stas/httpd/prefork/bin/httpd',
          'scheme' => 'http',
          'ssl_module_name' => 'mod_ssl',
          'port' => 8529,
          'sbindir' => '/home/stas/httpd/prefork/bin',
          't_conf' => '/home/stas/apache.org/modperl-2.0/t/conf',
          'servername' => 'localhost.localdomain',
          'inherit_documentroot' => '/home/stas/httpd/prefork/htdocs',
          'proxy' => 'off',
          'serveradmin' => 'you@your.address',
          'remote_addr' => '127.0.0.1',
          'perlpod' => '/home/stas/perl/ithread/lib/5.7.2/pod',
          'ssl_module' => 'mod_ssl.c',
          't_logs' => '/home/stas/apache.org/modperl-2.0/t/logs',
          'maxclients' => 1,
          'group' => 'stas',
          'thread_module_name' => 'worker',
          'documentroot' => '/home/stas/apache.org/modperl-2.0/t/htdocs',
          'serverroot' => '/home/stas/apache.org/modperl-2.0/t',
          'perl' => '/home/stas/perl/ithread/bin/perl',
          'src_dir' => '/home/stas/apache.org/modperl-2.0/src/modules/perl'

=head2 Threaded versus Non-threaded Perl Test's Compatibility

Since the tests are supposed to run properly under non-threaded and
threaded perl, you have to worry to enclose the threaded perl specific
configuration bits in:

  <IfDefine PERL_USEITHREADS>
      ... configuration bits
  </IfDefine>

C<Apache::Test> will start the server with -DPERL_USEITHREADS if the
Perl is ithreaded.

For example C<PerlOptions +Parent> is valid only for the threaded
perl, therefore you have to write:

  <IfDefine PERL_USEITHREADS>
      # a new interpreter pool
      PerlOptions +Parent
  </IfDefine>

Just like the configuration, the test's code has to work for both
versions as well. Therefore you should wrap the code specific to the
threaded perl into:

  if (have_perl 'ithreads'){
      # ithread specific code
  }

which is essentially does a lookup in $Config{useithreads}.

=head1 Debugging Tests

Sometimes your tests won't run properly or even worse will
segfault. There are cases where it's possible to debug broken tests
with simple print statements but usually it's very time consuming and
ineffective. Therefore it's a good idea to get yourself familiar with
Perl and C debuggers, and this knowledge will save you a lot of time
and grief in a long run.

=head2 Under C debugger

META: to be written

=head2 Under Perl debugger

When the Perl code misbehaves it's the best to run it under the Perl
debugger. Normally started as:

  % perl -d program.pl

the flow control gets passed to the Perl debugger, which allows you to
run the program in single steps and examine its states and variables
after every executed statement. Of course you can set up breakpoints
and watches to skip irrelevant code sections and watch after certain
variables. The I<perldebug> and the I<perldebtut> manpages are
covering the Perl debugger in fine details.

The C<Apache::Test> framework extends the Perl debugger and plugs in
C<LWP>'s debug features, so you can debug the requests. Let's take
test I<apache/read> from mod_perl 2.0 and present the features as we
go:

META: to be completed


=head1 Writing Tests Methodology

META: to be completed


=head2 When Tests Should Be Written

=over

=item * A New feature is Added

Every time a new feature is added new tests should be added to cover
the new feature.

=item * A Bug is Reported

Every time a bug gets reported, before you even attempt to fix the
bug, write a test that exposes the bug. This will make much easier for
you to test whether your fix actually fixes the bug.

Now fix the bug and make sure that test passes ok.

It's possible that a few tests can be written to expose the same
bug. Write them all -- the more tests you have the less chances are
that there is a bug in your code.

If the person reporting the bug is a programmer you may try to ask her
to write the test for you. But usually if the report includes a simple
code that reproduces the bug, it should probably be easy to convert
this code into a test.

=back




=head1 References

=over

=item * extreme programming methodology

Extreme Programming: A Gentle Introduction:
http://www.extremeprogramming.org/.

Extreme Programming: http://www.xprogramming.com/.

See also other sites linked from these URLs.

=back

=head1 Maintainers

Maintainer is the person(s) you should contact with updates,
corrections and patches.

Stas Bekman E<lt>stas (at) stason.orgE<gt>

=head1 Authors

Stas Bekman E<lt>stas (at) stason.orgE<gt>

Gary Benson E<lt>gbenson (at) redhat.comE<gt>

=cut

