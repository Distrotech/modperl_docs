=head1 NAME

Cute Tricks With Perl and Apache

=head1 Description

Perl and Apache play very well together, both for administration and
coding. However, adding mod_perl to the mix creates a heaven for an
administrator/programmer wanting to do cool things in no time!

=head1 Part I: Web Site Care and Feeding

These scripts are designed to make your life as a Webmaster easier,
leaving you time for more exciting things, like tango lessons.

=head2 Logs! Logs! Logs!

Left to their own devices, the log files will grow without limit,
eventually filling up your server's partition and bringing things to a
grinding halt.  But wait!  Don't turn off logging or throw them away.
Log files are your friends.

=head3 Log rotation

Script I.1.1 shows the basic script for rotating log files.  It
renames the current "access_log" to "access_log.0", "access_log.0" to
"access_log.1", and so on.  The oldest log gets deleted.  Run it from
a cron job to keep your log files from taking over.  The faster your
log files grow, the more frequently you should run the script.


  Script I.1.1: Basic Log File Rotation
  -------------------------------------
  #!/usr/local/bin/perl
  $LOGPATH='/usr/local/apache/logs';
  @LOGNAMES=('access_log','error_log','referer_log','agent_log');
  $PIDFILE = 'httpd.pid';
  $MAXCYCLE = 4;
  
  chdir $LOGPATH;  # Change to the log directory
  foreach $filename (@LOGNAMES) {
     for (my $s=$MAXCYCLE; $s >= 0; $s-- ) {
         $oldname = $s ? "$filename.$s" : $filename;
         $newname = join(".",$filename,$s+1);
         rename $oldname,$newname if -e $oldname;
     }
  }
  kill 'HUP',`cat $PIDFILE`;

=head3 Log rotation and archiving

But some people don't want to delete the old logs.  Wow, maybe some
day you could sell them for a lot of money to a marketing and
merchandising company!  Script I.1.2 appends the oldest to a gzip
archive.  Log files compress extremely well and make great bedtime
reading.


  Script I.1.2: Log File Rotation and Archiving
  ---------------------------------------------
  #!/usr/local/bin/perl
  $LOGPATH    = '/usr/local/apache/logs';
  $PIDFILE    = 'httpd.pid';
  $MAXCYCLE   = 4;
  $GZIP       = '/bin/gzip';
  
  @LOGNAMES=('access_log','error_log','referer_log','agent_log');
  %ARCHIVE=('access_log'=>1,'error_log'=>1);
  
  chdir $LOGPATH;  # Change to the log directory
  foreach $filename (@LOGNAMES) {
    system "$GZIP -c $filename.$MAXCYCLE >> $filename.gz" 
         if -e "$filename.$MAXCYCLE" and $ARCHIVE{$filename};
     for (my $s=$MAXCYCLE; $s >= 0; $s-- ) {
         $oldname = $s ? "$filename.$s" : $filename;
         $newname = join(".",$filename,$s+1);
         rename $oldname,$newname if -e $oldname;
     }
  }
  kill 'HUP',`cat $PIDFILE`;


=head3 Log rotation, compression and archiving

What's that?  Someone broke into your computer, stole your log files
and now B<he's> selling it to a Web marketing and merchandising
company?  Shame on them.  And on you for letting it happen.  Script
I.1.3 uses I<idea> (part of the SSLEay package) to encrypt the log
before compressing it.  You need GNU tar to run this one.  The log
files are individually compressed and encrypted, and stamped with the
current date.

  Script I.1.3: Log File Rotation and Encryption
  ----------------------------------------------
  #!/usr/local/bin/perl
  use POSIX 'strftime';
  
  $LOGPATH     = '/home/www/logs';
  $PIDFILE     = 'httpd.pid';
  $MAXCYCLE    = 4;
  $IDEA        = '/usr/local/ssl/bin/idea';
  $GZIP        = '/bin/gzip';
  $TAR         = '/bin/tar';
  $PASSWDFILE  = '/home/www/logs/secret.passwd';
  
  @LOGNAMES=('access_log','error_log','referer_log','agent_log');
  %ARCHIVE=('access_log'=>1,'error_log'=>1);
  
  chdir $LOGPATH;  # Change to the log directory
  foreach $filename (@LOGNAMES) {
      my $oldest = "$filename.$MAXCYCLE";
      archive($oldest) if -e $oldest and $ARCHIVE{$filename};
      for (my $s=$MAXCYCLE; $s >= 0; $s-- ) {
          $oldname = $s ? "$filename.$s" : $filename;
          $newname = join(".",$filename,$s+1);
          rename $oldname,$newname if -e $oldname;
      }
  }
  kill 'HUP',`cat $PIDFILE`;
  
  sub archive {
      my $f = shift;
      my $base = $f;
      $base =~ s/\.\d+$//;
      my $fn = strftime("$base.%Y-%m-%d_%H:%M.gz.idea",localtime);
      system "$GZIP -9 -c $f | $IDEA -kfile $PASSWDFILE > $fn";
      system "$TAR rvf $base.tar --remove-files $fn";
  }

=head3 Log Parsing

There's a lot you can learn from log files.  Script I.1.4 does the
basic access log regular expression match.  What you do with the
split-out fields is limited by your imagination.  Here's a typical log
entry so that you can follow along (wrapped for readability):

  portio.cshl.org - - [03/Feb/1998:17:42:15 -0500]  
       "GET /pictures/small_logo.gif HTTP/1.0" 200 2172

  Script I.1.4: Basic Log Parsing
  -------------------------------
  #!/usr/local/bin/perl
  
  $REGEX=/^(\S+) (\S+) (\S+) \[([^]]+)\] "(\w+) (\S+).*" (\d+) (\S+)/;
  while (<>) {
     ($host,$rfc931,$user,$date,$request,$URL,$status,$bytes) = m/$REGEX/o;
      &collect_some_statistics;
  }
  &print_some_statistics;
  
  sub collect_some_statistics {
    # for you to fill in
  }
 
  sub print_some_statistics {
    # for you to fill in
  }


Script I.1.5 scans the log for certain status codes and prints out the
top URLs or hosts that triggered them.  It can be used to get
quick-and-dirty usage statistics, to find broken links, or to detect
certain types of breakin attempts.  Use it like this:

 % find_status.pl -t10 200 ~www/logs/access_log

  TOP 10 URLS/HOSTS WITH STATUS CODE 200:

    REQUESTS  URL/HOST
    --------  --------
      1845    /www/wilogo.gif
      1597    /cgi-bin/contig/sts_by_name?database=release
      1582    /WWW/faqs/www-security-faq.html
      1263    /icons/caution.xbm
       930    /
       886    /ftp/pub/software/WWW/cgi_docs.html
       773    /cgi-bin/contig/phys_map
       713    /icons/dna.gif
       686    /WWW/pics/small_awlogo.gif


  Script I.1.5: Find frequent status codes
  ----------------------------------------
  #!/usr/local/bin/perl
  # File: find_status.pl
  
  require "getopts.pl";
  &Getopts('L:t:h') || die <<USAGE;
  Usage: find_status.pl [-Lth] <code1> <code2> <code3> ...
         Scan Web server log files and list a summary
         of URLs whose requests had the one of the
         indicated status codes.
  Options:
         -L <domain>  Ignore local hosts matching this domain
         -t <integer> Print top integer URLS/HOSTS [10]
         -h           Sort by host rather than URL
  USAGE
      ;
  if ($opt_L) {
      $opt_L=~s/\./\\./g;
      $IGNORE = "(^[^.]+|$opt_L)\$";
  }
  $TOP=$opt_t || 10;
  
  while (@ARGV) {
      last unless $ARGV[0]=~/^\d+$/;
      $CODES{shift @ARGV}++;
  }
  
  while (<>) {
      ($host,$rfc931,$user,$date,$request,$URL,$status,$bytes) =
          /^(\S+) (\S+) (\S+) \[([^]]+)\] "(\w+) (\S+).*" (\d+) (\S+)/;
      next unless $CODES{$status};
      next if $IGNORE && $host=~/$IGNORE/io;
      $info = $opt_h ? $host : $URL;
      $found{$status}->{$info}++;
  }
  
  foreach $status (sort {$a<=>$b;} sort keys %CODES) {
      $info = $found{$status};
      $count = $TOP;
      foreach $i (sort {$info->{$b} <=> $info->{$a};} keys %{$info}) {
          write;
          last unless --$count;
      }
      $- = 0;  # force a new top-of-report
  }
  
  format STDOUT_TOP=
  
  TOP @## URLS/HOSTS WITH STATUS CODE @##:
      $TOP,                      $status
  
      REQUESTS  URL/HOST
      --------  --------
  .
  format STDOUT=
      @#####    @<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
      $info->{$i},$i
  .

=head3 Offline Reverse DNS Resolution

Many sites turn off reverse name look-ups in order to improve server
performance.  The log files will contain the IP addresses of remote
hosts, but not their DNS names.  Script I.1.6 will do the reverse name
resolution off-line.  You can run it before the log rotation and
archiving scripts, preferably on a machine that isn't busy serving Web
requests at the same time.

This script maintains a cache of resolved names.  Because performance
is more important than completeness, if an address doesn't resolve
after two seconds, it moves on to the next one and never tries that
name again.

  Script I.1.6: Reverse DNS Resolution
  ------------------------------------
  #!/usr/local/bin/perl
  
  use constant TIMEOUT => 2;
  $SIG{ALRM} = sub {die "timeout"};
  
  while (<>) {
      s/^(\S+)/lookup($1)/e;
  } continue { 
      print;
  }
  
  sub lookup {
      my $ip = shift;
      return $ip unless $ip=~/\d+\.\d+\.\d+\.\d+/;
      return $CACHE{$ip} if exists $CACHE{$ip};
      my @h = eval <<'END';
      alarm(TIMEOUT);
      my @i = gethostbyaddr(pack('C4',split('\.',$ip)),2);
      alarm(0);
      @i;
  END
      $CACHE{$ip} = $h[0];
      return $CACHE{$ip} || $ip;
  }

=head3 Detecting Robots

I was very upset a few months ago when I did some log analysis and
discovered that 90% of my hits were coming from 10% of users, and that
those 10% were all robots!  Script I.1.7 is the script I used to
crunch the log and perform the analysis.  The script works like this:

=over

=item 1

we assume that anyone coming from the same IP address with the same
user agent within 30 minutes is the same person/robot (not quite
right, but close enough).

=item 2

anything that fetches /robots.txt is probably a robot, and a "polite"
one, to boot.

=item 3

we count the total number of accesses a user agent makes.

=item 4

we average the interval between successive fetches.

=item 5

we calculate an "index" which is the number of hits over the interval.
Robots have higher indexes than people.

=item 6

we print everything out in a big tab-delimited table for graphing.

=back

By comparing the distribution of "polite" robots to the total
distribution, we can make a good guess as to who the impolite robots
are.

  Script I.1.7: Robo-Cop
  ----------------------
  #!/usr/local/bin/perl
  
  use Time::ParseDate;
  use strict 'vars';
  
  # after 30 minutes, we consider this a new session
  use constant MAX_INTERVAL => 60*30;  
  my (%HITS,%INT_NUMERATOR,%INT_DENOMINATOR,%POLITE,%LAST,$HITS);
  
  # This uses a non-standard agent log with lines formatted like this:
  # [08/Feb/1998:12:28:35 -0500] phila249-pri.voicenet.com "Mozilla/3.01 (Win95; U)" /cgi-bin/fortune
  
  my $file = shift;
  open (IN,$file=~/\.gz$/ ? "zcat $file |" : $file ) || die "Can't open file/pipe: $!"; 
  
  while (<IN>) {
      my($date,$host,$agent,$URL) = /^\[(.+)\] (\S+) "(.*)" (\S+)$/;
      next unless $URL=~/\.(html|htm|txt)$/;
  
      $HITS++;
      $host = "$host:$agent"; # concatenate host and agent
      $HITS{$host}++;
      my $seconds = parsedate($date);
      if ($LAST{$host}) {
         my $interval = $seconds - $LAST{$host};
 	 if ($interval < MAX_INTERVAL) {
 	     $INT_NUMERATOR{$host} += $interval;
 	     $INT_DENOMINATOR{$host}++;
 	 }
      }
      $LAST{$host} = $seconds;
      $POLITE{$host}++ if $URL eq '/robots.txt';
      print STDERR $HITS,"\n" if ($HITS % 1000) == 0;
  }
 
  # print out, sorted by hits
  print join("\t",qw/Client Robot Hits Interval Hit_Percent Index/),"\n";
  foreach (sort {$HITS{$b}<=>$HITS{$a}} keys %HITS) {
      next unless $HITS{$_} >= 5;             # not enough total hits to mean much
      next unless $INT_DENOMINATOR{$_} >= 5;  # not enough consecutive hits to mean much
 
      my $mean_interval = $INT_NUMERATOR{$_}/$INT_DENOMINATOR{$_};
      my $percent_hits = 100*($HITS{$_}/$HITS);
      my $index = $percent_hits/$mean_interval;
  
      print join("\t",
 	        $_,
 	        $POLITE{$_} ? 'yes' : 'no',
 	        $HITS{$_},
 	        $mean_interval,
 	        $percent_hits,
 	        $index
                ),"\n";
  }

=head3 Logging to syslog

If you run a large site with many independent servers, you might be
annoyed that they all log into their own file systems rather than into
a central location.  Apache offers a little-known feature that allows
it to send its log entries to a process rather than a file.  The
process (a Perl script, natch) can do whatever it likes with the logs.
For instance, using Tom Christiansen's C<Syslog> module to send the
info to a remote syslog daemon.

Here's what you add to the Apache httpd.conf file:

  <VirtualHost www.company1.com>
     CustomLog "| /usr/local/apache/bin/logger company1" common
     # blah blah
  </VirtualHost>
  
  <VirtualHost www.company2.com>
     CustomLog "| /usr/local/apache/bin/logger company2" common
     # blah blah
  </VirtualHost>

Do the same for each server on the local network.  

Here's what you add to each Web server's syslog.conf (this assumes
that the central logging host has the alias hostname "loghost":

  local0.info			@loghost

Here's what you add to the central log host's syslog.conf:

  local0.info			/var/log/web/access_log

Script I.1.8 shows the code for the "logger" program:

  Script I.1.8 "logger"
  ---------------------
  #!/usr/local/bin/perl
  # script: logger
  
  use Sys::Syslog;
  
  $SERVER_NAME = shift || 'www';
  $FACILITY = 'local0';
  $PRIORITY = 'info';
  
  Sys::Syslog::setlogsock('unix');
  openlog ($SERVER_NAME,'ndelay',$FACILITY);
  while (<>) {
      chomp;
      syslog($PRIORITY,$_);
  }
  closelog;

=head3 Logging to a relational database

One of the selling points of the big commercial Web servers is that
they can log to relational databases via ODBC.  Big whoop.  With a
little help from Perl, Apache can do that too.  Once you've got the
log in a relational database, you can data mine to your heart's
content.

This example uses the freeware mySQL DBMS.  To prepare, create an
appropriate database containing a table named "access_log".  It should
have a structure like this one.  Add whatever indexes you think you
need.  Also notice that we truncate URLs at 255 characters.  You might
want to use TEXT columns instead.

  CREATE TABLE access_log (
        when    datetime     not null,
        host	varchar(255) not null,
        method  char(4)      not null,
        url	varchar(255) not null,
        auth	varchar(50),
        browser varchar(50),
        referer varchar(255),
        status  smallint(3)  not null,
        bytes	int(8)       default 0
  );

Now create the following entries in httpd.conf:

  LogFormat "\"%{%Y-%m-%d %H:%M:%S}t\" %h \"%r\" %u \"%{User-agent}i\" %{Referer}i %s %b" mysql
  CustomLog "| /usr/local/apache/bin/mysqllog" mysql

Script I.1.9 is the source code for mysqllog.

  Script I.1.9 "mysqllog"
  -----------------------
  #!/usr/local/bin/perl
  # script: mysqllog
  use DBI;
  
  use constant DSN       => 'dbi:mysql:www';
  use constant DB_TABLE  => 'access_log';
  use constant DB_USER   => 'nobody';
  use constant DB_PASSWD => '';
  
  $PATTERN = '"([^"]+)" (\S+) "(\S+) (\S+) [^"]+" (\S+) "([^"]+)" (\S+) (\d+) (\S+)';
 
  $db = DBI->connect(DSN,DB_USER,DB_PASSWD) || die DBI->errstr;
  $sth = $db->prepare("INSERT INTO ${\DB_TABLE} VALUES(?,?,?,?,?,?,?,?,?)") 
      || die $db->errstr;
  while (<>) {
      chomp;
      my($date,$host,$method,$url,$user,$browser,$referer,$status,$bytes) = /$PATTERN/o;
      $user    = undef if $user    eq '-';
      $referer = undef if $referer eq '-';
      $browser = undef if $browser eq '-';
      $bytes   = undef if $bytes   eq '-';
      $sth->execute($date,$host,$method,$url,$user,$browser,$referer,$status,$bytes);
  }
  $sth->finish;
  $db->disconnect;

NOTE: Your database will grow very quickly.  Make sure that you have a
plan for truncating or archiving the oldest entries.  Or have a lot of
storage space handy!  Also be aware that this will cause a lot of
traffic on your LAN.  Better start shopping around for 100BT hubs.

=head2 My server fell down and it can't get up!

Web servers are very stable and will stay up for long periods of time
if you don't mess with them.  However, human error can bring them
down, particularly if you have a lot of developers and authors
involved in running the site.  The scripts in this section watch the
server and send you an email message when there's a problem.

=head3 Monitoring a local server

The simplest script just tries to signal the Web server process.  If
the process has gone away, it sends out an S.O.S.  See script I.2.1
shows the technique.  Notice that the script has to run as I<root> in
order to successfully signal the server.

  Script I.2.1 "localSOS"
  -----------------------
  #!/usr/local/bin/perl
  # script: localSOS
  
  use constant PIDFILE  => '/usr/local/apache/var/run/httpd.pid';
  $MAIL                 =  '/usr/sbin/sendmail';
  $MAIL_FLAGS           =  '-t -oi';
  $WEBMASTER            =  'webmaster';
  
  open (PID,PIDFILE) || die PIDFILE,": $!\n";
  $pid = <PID>;  close PID;
  kill 0,$pid || sos();
  
  sub sos {
    open (MAIL,"| $MAIL $MAIL_FLAGS") || die "mail: $!";
    my $date = localtime();
    print MAIL <<END;
  To: $WEBMASTER
  From: The Watchful Web Server Monitor <nobody>
  Subject: Web server is down
  
  I tried to call the Web server at $date but there was
  no answer.
  
  Respectfully yours,
  
  The Watchful Web Server Monitor   
  END
    close MAIL;
  }


=head3 Monitoring a remote server

Local monitoring won't catch problems with remote machines, and
they'll miss subtle problems that can happen when the Web server hangs
but doesn't actually crash.  A functional test is better.  Script
I.2.2 uses the LWP library to send a HEAD request to a bunch of
servers.  If any of them fails to respond, it sends out an SOS.  This
script does B<not> have to run as a privileged user.

  Script I.2.2 "remoteSOS"
  ------------------------
  #!/usr/local/bin/perl
  # script: remoteSOS
  
  use LWP::Simple;
  %SERVERS = (
	 "Fred's server"   => 'http://www.fred.com',
	 "Martha's server" => 'http://www.stewart-living.com',
	 "Bill's server"   => 'http://www.whitehouse.gov'
	 );
  $MAIL                 =  '/usr/sbin/sendmail';
  $MAIL_FLAGS           =  '-t -oi';
  $WEBMASTER            =  'webmaster';
  
  foreach (sort keys %SERVERS) {
     sos($_) unless head($SERVERS{$_});
  }
  
  sub sos {
    my $server = shift;
    open (MAIL,"| $MAIL $MAIL_FLAGS") || die "mail: $!";
    my $date = localtime();
    print MAIL <<END;
  To: $WEBMASTER
  From: The Watchful Web Server Monitor <nobody>
  Subject: $server is down
  
  I tried to call $server at $date but there was
  no one at home.
  
  Respectfully yours,
  
  The Watchful Web Server Monitor   
  END
    close MAIL;
  }

=head3 Resurrecting Dead Servers

So it's not enough to get e-mail that the server's down, you want to
relaunch it as well?  Script I.2.3 is a hybrid of localSOS and
remoteSOS that tries to relaunch the local server after sending out
the SOS.  It has to be run as B<root>, unless you've made I<apachectl>
suid to root.

  Script I.2.2 "webLazarus"
  ------------------------- 
  #!/usr/local/bin/perl
  # script: webLazarus
  
  use LWP::Simple;
  use constant URL       => 'http://presto.capricorn.com/';
  use constant APACHECTL => '/usr/local/apache/bin/apachectl';
  $MAIL                  =  '/usr/sbin/sendmail';
  $MAIL_FLAGS            =  '-t -oi';
  $WEBMASTER             =  'lstein@prego.capricorn.com';
  
  head(URL) || resurrect();
  
  sub resurrect {
      open (STDOUT,"| $MAIL $MAIL_FLAGS") || die "mail: $!";
      select STDOUT; $| = 1;
      open (STDERR,">&STDOUT");
  
      my $date = localtime();
      print <<END;
  To: $WEBMASTER
  From: The Watchful Web Server Monitor <nobody>
  Subject: Web server is down
  
  I tried to call the Web server at $date but there was
  no answer.  I am going to try to resurrect it now:
  
  Mumble, mumble, mumble, shazzzzammmm!
  
  END
      ;
  
      system APACHECTL,'restart';
      
      print <<END;
 
  That's the best I could do.  Hope it helped.
  
  Worshipfully yours,
  
  The Web Monitor
  END
      close STDERR;
      close STDOUT;
  }

Here's the message you get when the script is successful:

  Date: Sat, 4 Jul 1998 14:55:38 -0400
  To: lstein@prego.capricorn.com
  Subject: Web server is down
  
  I tried to call the Web server at Sat Jul  4 14:55:37 1998 but there was
  no answer.  I am going to try to resurrect it now:
  
  Mumble, mumble, mumble, shazzzzammmm!
  
  /usr/local/apache/bin/apachectl restart: httpd not running, trying to start
  [Sat Jul  4 14:55:38 1998] [debug] mod_so.c(258): loaded module setenvif_module
  [Sat Jul  4 14:55:38 1998] [debug] mod_so.c(258): loaded module unique_id_module
  /usr/local/apache/bin/apachectl restart: httpd started
  
  That's the best I could do.  Hope it helped.
  
  Worshipfully yours,
  
  The Web Monitor

=head2 Site Replication and Mirroring

Often you will want to mirror a page or set of pages from another
server, for example, to distribute the load amongst several replicate
servers, or to keep a set of reference pages handy.  The LWP library
makes this easy.

=head3 Mirroring Single Pages

  % ./MirrorOne.pl
  cats.html: Not Modified
  dogs.html: OK
  gillie_fish.html: Not Modified

  Script I.3.1 mirrorOne.pl
  -------------------------
  #!/usr/local/bin/perl
  # mirrorOne.pl
  
  use LWP::Simple;
  use HTTP::Status;
  
  use constant DIRECTORY => '/local/web/price_lists';
  %DOCUMENTS = (
	 'dogs.html'  => 'http://www.pets.com/dogs/price_list.html',
	 'cats.html'  => 'http://www.pets.com/cats/price_list.html',
	 'gillie_fish.html' => 'http://aquaria.com/prices.html'
	 );
  chdir DIRECTORY;
  foreach (sort keys %DOCUMENTS) {
     my $status = mirror($DOCUMENTS{$_},$_);
     warn "$_: ",status_message($status),"\n";
  }

=head3 Mirroring a Document Tree

With a little more work, you can recursively mirror an entire set of
linked pages.  Script I.3.2 mirrors the requested document and all
subdocuments, using the LWP C<HTML::LinkExtor> module to extract all
the HTML links.

  Script I.3.2 mirrorTree.pl
  --------------------------
  #!/usr/local/bin/perl
  
  # File: mirrorTree.pl
  
  use LWP::UserAgent;
  use HTML::LinkExtor;
  use URI::URL;
  use File::Path;
  use File::Basename;
  %DONE    = ();
  
  my $URL = shift;
  
  $UA     = new LWP::UserAgent;
  $PARSER = HTML::LinkExtor->new();
  $TOP    = $UA->request(HTTP::Request->new(HEAD => $URL));
  $BASE   = $TOP->base;
  
  mirror(URI::URL->new($TOP->request->url));
  
  sub mirror {
      my $url = shift;
  
      # get rid of query string "?" and fragments "#"
      my $path = $url->path;
      my $fixed_url = URI::URL->new ($url->scheme . '://' . $url->netloc . $path);
  
      # make the URL relative
      my $rel = $fixed_url->rel($BASE);
      $rel .= 'index.html' if $rel=~m!/$! || length($rel) == 0;
  
      # skip it if we've already done it
      return if $DONE{$rel}++;
  
      # create the directory if it doesn't exist already
      my $dir = dirname($rel);
      mkpath([$dir]) unless -d $dir;
  
      # mirror the document
      my $doc = $UA->mirror($fixed_url,$rel);
      print STDERR "$rel: ",$doc->message,"\n";
      return if $doc->is_error;
  
      # Follow HTML documents
      return unless $rel=~/\.html?$/i;
      my $base = $doc->base;
     
      # pull out the links and call us recursively
      my @links = $PARSER->parse_file("$rel")->links;
      my @hrefs = map { url($_->[2],$base)->abs } @links;
  
      foreach (@hrefs) {
          next unless is_child($BASE,$_);
 	  mirror($_);
      }
  
  }
  
  sub is_child {
      my ($base,$url) = @_;
      my $rel = $url->rel($base);
      return ($rel ne $url) && ($rel !~ m!^[/.]!);
  }

=head3 Checking for Bad Links

A slight modification of this last script allows you to check an
entire document hierarchy (your own or someone else's) for bad links.
The script shown in I.3.3 traverses a document, and checks each of the
http:, ftp: and gopher: links to see if there's a response at the
other end.  Links that point to sub-documents are fetched and
traversed as before, so you can check your whole site in this way.

  % find_bad_links http://prego/apache-1.2/
  checking http://prego/apache-1.2/...
  checking http://prego/apache-1.2/manual/...
  checking http://prego/apache-1.2/manual/misc/footer.html...
  checking http://prego/apache-1.2/manual/misc/header.html...
  checking http://prego/apache-1.2/manual/misc/nopgp.html...
  checking http://www.yahoo.com/Science/Mathematics/Security_and_Encryption/...
  checking http://www.eff.org/pub/EFF/Policy/Crypto/...
  checking http://www.quadralay.com/www/Crypt/Crypt.html...
  checking http://www.law.indiana.edu/law/iclu.html...
  checking http://bong.com/~brian...
  checking http://prego/apache-1.2/manual/cgi_path.html...
  checking http://www.ics.uci.edu/pub/ietf/http/...
    . 
    . 
    .
  BAD LINKS:
  manual/misc/known_bugs.html : http://www.apache.org/dist/patches/apply_to_1.2b6/
  manual/misc/fin_wait_2.html : http://www.freebsd.org/
  manual/misc/fin_wait_2.html : http://www.ncr.com/
  manual/misc/compat_notes.html : http://www.eit.com/
  manual/misc/howto.html : http://www.zyzzyva.com/robots/alert/
  manual/misc/perf.html : http://www.software.hp.com/internet/perf/tuning.html
  manual/misc/perf.html : http://www.qosina.com/~awm/apache/linux-tcp.html
  manual/misc/perf.html : http://www.sun.com/sun-on-net/Sun.Internet.Solutions/performance/
  manual/misc/perf.html : http://www.sun.com/solaris/products/siss/
  manual/misc/nopgp.html : http://www.yahoo.com/Science/Mathematics/Security_and_Encryption/
  
  152 documents checked
  11 bad links

  Script I.3.2 find_bad_links.pl
  ------------------------------
  #!/usr/local/bin/perl
  
  # File: find_bad_links.pl
   
  use LWP::UserAgent;
  use HTML::LinkExtor;
  use URI::URL;
  use WWW::RobotRules;
  
  %CAN_HANDLE = ('http'=>1,
 	        'gopher'=>1,
 	        # 'ftp'=>1,   # timeout problems?
 	        );
  %OUTCOME = ();
  $CHECKED = $BAD = 0;
  @BAD = ();
  
  my $URL = shift;
  
  $UA     = new LWP::UserAgent;
  $PARSER = HTML::LinkExtor->new();
  $TOP    = $UA->request(HTTP::Request->new(HEAD => $URL));
  $BASE   = $TOP->base;
  
  # handle robot rules
  my $robots = URI::URL->new('robots.txt',$BASE->scheme.'://'.$BASE->netloc);
  my $robots_text = $UA->request(HTTP::Request->new(GET=>$robots))->content;
  $ROBOTRULES = WWW::RobotRules->new;
  $ROBOTRULES->parse($robots->abs,$robots_text);
  
  check_links(URI::URL->new($TOP->request->url));
  if (@BAD) {
      print "\nBAD LINKS:\n";
      print join("\n",@BAD),"\n\n";
  }
  print "$CHECKED documents checked\n",scalar(@BAD)," bad links\n";
  
  sub check_links {
      my $url = shift;
      my $fixed_url = $url;
      $fixed_url =~ s/\#.+$//;
      
      return 1 unless $CAN_HANDLE{$url->scheme};
      
      # check cached outcomes
      return $OUTCOME{$fixed_url} if exists $OUTCOME{$fixed_url};
      
      print STDERR "checking $fixed_url...\n";
      $CHECKED++;
      
      my $rel = $url->rel($BASE) || 'index.html';
      my $child = is_child($BASE,$url);
      $UA->timeout(5);
      my $doc = $d = $UA->request(HTTP::Request->new(($child ? 'GET' : 'HEAD' )=>$url));
      $OUTCOME{$fixed_url} = $doc->is_success;
      
      return $OUTCOME{$fixed_url} 
      unless $ROBOTRULES->allowed($fixed_url) 
        && $child && $doc->header('Content-type') eq 'text/html';
      
      # Follow HTML documents
      my $base = $doc->base;
      
      # pull out the links and call us recursively
      my @links = $PARSER->parse($doc->content)->links;
      my @hrefs = map { url($_->[2],$base)->abs } @links;
      
      foreach (@hrefs) {
 	 next if check_links($_);
 	 push (@BAD,"$rel : $_");
      }
      1;
  }
  
  sub is_child {
      my ($base,$url) = @_;
      my $rel = $url->rel($base);
      return ($rel ne $url) && ($rel !~ m!^[/.]!);
  }

=head2 Load balancing

You've hit the big time, and your site is getting more hits than you
ever dreamed of.  Millions, zillions of hits.  What's that?  System
load just passed 50 and response time is getting kinda' s-l-o-w-w-w?

Perl to the rescue.  Set up several replica Web servers with different
hostnames and IP addresses.  Run this script on the "main" site and
watch it round-robin the requests to the replica servers.  It uses
C<IO::Socket> to listen for incoming requests on port 80.  It then
changes its privileges to run as nobody.nogroup, just like a real Web
server.  Next it preforks itself a few times (and you always thought
preforking was something fancy, didn't you?), and goes into an
C<accept()> loop.  Each time an incoming session comes in, it forks
off another child to handle the request.  The child reads the HTTP
request and issues the an HTTP redirection to send the browser to a
randomly selected server.

NOTE: Another way to do this is to have multiple "A" records defined
for your server's hostname and let DNS caching distribute the load.

  Script I.4.1: A Load Balancing "Web Server"
  -------------------------------------------
  #!/usr/local/bin/perl
  
  # list of hosts to balance between
  @HOSTS = qw/www1.web.org www2.web.org www3.web.org www4.web.org/;
  
  use IO::Socket;
  $SIG{CHLD} = sub { wait() };
  $ENV{'PATH'}='/bin:/usr/bin';
  chomp($hostname = `/bin/hostname`);
  
  # Listen on port 80
  $sock = IO::Socket::INET->new(Listen  => 5,
 	                        LocalPort => 80,
                                LocalAddr => $hostname,
                                Reuse     => 1,
                                Proto    => 'tcp');
  
  # become "nobody"
  $nobody  = (getpwnam('nobody'))[2]  || die "nobody is nobody";
  $nogroup = (getgrnam('nogroup'))[2] || die "can't grok nogroup";
  ($<,$() = ($>,$)) = ($nobody,$nogroup); # get rid of root privileges!
  ($\,$/) = ("\r\n","\r\n\r\n");          # CR/LF on output/input
  
  # Go into server mode
  close STDIN; close STDOUT; close STDERR;
  
  # prefork -- gee is that all there is to it?
  fork() && fork() && fork() && fork() && exit 0;
  
  # start accepting connections
  while (my $s = $sock->accept()) {
      do { $s->close; next; } if fork();
      my $request = <$s>;
      redirect($1,$s) if $request=~/(?:GET|POST|HEAD|PUT)\s+(\S+)/;
      $s->flush;
      undef $s;
      exit 0;
  }
  
  sub redirect {
      my ($url,$s) = @_;
      my $host = $HOSTS[rand(@HOSTS)];
      print $s "HTTP/1.0 301 Moved Temporarily";
      print $s "Server: Lincoln's Redirector/1.0";
      print $s "Location: http://${host}${url}";
      print $s "";
  }

=head2 Torture Testing a Server

Any server written in C suffers the risk of static buffer overflow
bugs.  In the past, these bugs have led to security compromises and
Web server breakins.  Script I.2.3 torture tests servers and CGI
scripts by sending large amounts of random date to them.  If the
server crashes, it probably contains a buffer overflow bug.

Here's what you see when a server crashes:

  % torture.pl -t 1000 -l 5000 http://www.capricorn.com
  torture.pl version 1.0 starting
  Base URL:               http://www.capricorn.com/cgi-bin/search
  Max random data length: 5000
  Repetitions:            1000
  Post:                   0
  Append to path:         0
  Escape URLs:            0
  
  200 OK
  200 OK
  200 OK
  200 OK
  200 OK
  500 Internal Server Error
  500 Could not connect to www.capricorn.com:80
  500 Could not connect to www.capricorn.com:80
  500 Could not connect to www.capricorn.com:80

  Script I.5.1: torture tester
  ----------------------------
  #!/usr/local/bin/perl
  
  # file: torture.pl
  # Torture test Web servers and scripts by sending them large arbitrary URLs
  # and record the outcome.
  
  use LWP::UserAgent;
  use URI::Escape 'uri_escape';
  require "getopts.pl";
  
  $USAGE = <<USAGE;
  Usage: $0 -[options] URL
  Torture-test Web servers and CGI scripts
  
  Options:
   -l <integer>  Max length of random URL to send [1024 bytes]
   -t <integer>  Number of times to run the test [1]
   -P            Use POST method rather than GET method
   -p            Attach random data to path rather than query string
   -e            Escape the query string before sending it
  USAGE
  
  $VERSION = '1.0';
  
  # process command line
  &Getopts('l:t:Ppe') || die $USAGE;
  
  # get parameters
  $URL    = shift || die $USAGE;
  $MAXLEN = $opt_l ne '' ? $opt_l : 1024;
  $TIMES  = $opt_t || 1;
  $POST   = $opt_P || 0;
  $PATH   = $opt_p || 0;
  $ESCAPE = $opt_e || 0;
  
  # cannot do both a post and a path at the same time
  $POST = 0 if $PATH;
  
  # create an LWP agent
  my $agent = new LWP::UserAgent;
  
  print <<EOF;
  torture.pl version $VERSION starting
  Base URL:               $URL
  Max random data length: $MAXLEN
  Repetitions:            $TIMES
  Post:                   $POST
  Append to path:         $PATH
  Escape URLs:            $ESCAPE
  
  EOF
  
  
  # Do the test $TIMES times
  while ($TIMES) {
      # create a string of random stuff
      my $garbage = random_string(rand($MAXLEN));
      $garbage = uri_escape($garbage) if $ESCAPE;
      my $url = $URL;
      my $request;
  
      if (length($garbage) == 0) { # if no garbage to add, just fetch URL
         $request = new HTTP::Request ('GET',$url);
      }
  
      elsif ($POST) {		# handle POST request
         my $header = new HTTP::Headers (
                                        Content_Type => 'application/x-www-form-urlencoded',
                                        Content_Length => length($garbage)
                                        );
        # garbage becomes the POST content
        $request = new HTTP::Request ('POST',$url,$header,$garbage);
        
      } else {			# handle GET request
        
         if ($PATH) {		# append garbage to the base URL
             chop($url) if substr($url,-1,1) eq '/'; 
             $url .= "/$garbage";
         } else {		# append garbage to the query string
             $url .= "?$garbage";
         }
         
         $request = new HTTP::Request ('GET',$url);
      }
      
      # do the request and fetch the response
      my $response = $agent->request($request);
      
      # print the numeric response code and the message
      print $response->code,' ',$response->message,"\n";
  
  } continue { $TIMES-- }
  
  # return some random data of the requested length
  sub random_string {
      my $length = shift;
      return undef unless $length >= 1;
      return join('',map chr(rand(255)),0..$length-1);
  }

For other load testing tools, have a look at our L<Benchmarking
section|guide::performance/Essential_Tools>.

=head1 Part II: mod_perl -- Faster Than a Speeding Bullet

I<mod_perl> is Doug MacEachern's embedded Perl for Apache.  With a
mod_perl-enabled server, there's no tedious waiting around while the
Perl interpreter fires up, reads and compiles your script.  It's right
there, ready and waiting.  What's more, once compiled your script
remains in memory, all charged and raring to go.  Suddenly those
sluggish Perl CGI scripts race along at compiled C speeds...or so it
seems.

Most CGI scripts will run unmodified under mod_perl using the
C<Apache::Registry>x CGI compatability layer.  But that's not the whole
story.  The exciting part is that mod_perl gives you access to the
Apache API, letting you get at the innards of the Apache server and
change its behavior in powerful and interesting ways.  This section
will give you a feel for the many things that you can do with
mod_perl.

=head2 Creating Dynamic Pages

This is a ho-hum because you can do it with CGI and with
C<Apache::Registry>.  Still, it's worth seeing a simple script written
using the strict mod_perl API so you see what it looks like.  Script
II.1.1 prints out a little hello world message.

Install it by adding a section like this one to one of the
configuration files:

  <Location /hello/world>
    SetHandler  perl-script
    PerlHandler Apache::Hello
  </Location>

  Script II.1.1 Apache::Hello
  ---------------------------
  package Apache::Hello;
  # file: Apache/Hello.pm
  
  use strict vars;
  use Apache::Constants ':common';
  
  sub handler {
      my $r = shift;
      $r->content_type('text/html');
      $r->send_http_header;
      my $host = $r->get_remote_host;
      $r->print(<<END);
  <html>
  <head>
  <title>Hello There</title>
  </head>
  <body>
  <h1>Hello $host</h1>
  Hello to all the nice people at the Perl conference.  Lincoln is
  trying really hard.  Be kind.
  </body>
  </html>
  END
      return OK;
  }
  1;

You can do all the standard CGI stuff, such as reading the query
string, creating fill-out forms, and so on.  In fact, C<CGI.pm> works
with mod_perl, giving you the benefit of sticky forms, cookie
handling, and elegant HTML generation.

=head2 File Filters

This is where the going gets fun.  With mod_perl, you can install a
I<content handler> that works a lot like a four-letter word
starrer-outer, but a lot faster.

=head3 Adding a Canned Footer to Every Page

Script II.2.1 adds a canned footer to every HTML file.  The footer
contains a copyright statement, plus the modification date of the
file.  You could easily extend this to add other information, such as
a page hit counter, or the username of the page's owner.

This can be installed as the default handler for all files in a
particular subdirectory like this:

  <Location /footer>
    SetHandler perl-script
    PerlHandler Apache::Footer
  </Location>


Or you can declare a new ".footer" extension and arrange for all files
with this extension to be passed through the footer module:

  AddType text/html .footer
  <Files ~ "\.footer$">
     SetHandler  perl-script
     PerlHandler Apache::Footer
  </Files>


  Script II.2.1 Apache::Footer
  ----------------------------
  package Apache::Footer;
  # file Apache::Footer.pm
  
  use strict vars;
  use Apache::Constants ':common';
  use IO::File;
  
  sub handler {
      my $r = shift;
      return DECLINED unless $r->content_type() eq 'text/html';
      my $file = $r->filename;
      return DECLINED unless $fh=IO::File->new($file);
      my $modtime = localtime((stat($file))[9]);
      my $footer=<<END;
  <hr>
  &copy; 1998 <a href="http://www.ora.com/">O\'Reilly &amp; Associates</a><br>
  <em>Last Modified: $modtime</em>
  END
  
      $r->send_http_header;
     
      while (<$fh>) {
          s!(</BODY>)!$footer$1!oi;
      } continue {
	 $r->print($_);
      }
  
      return OK;
  }
  
  1;

For more customized footer/header handling, you might want to look at
the C<Apache::Sandwich> module on CPAN.

=head3 Dynamic Navigation Bar

Sick of hand-coding navigation bars in every HTML page?  Less than
enthused by the Java & JavaScript hacks?  Here's a dynamic navigation
bar implemented as a server side include.

First create a global configuration file for your site.  The first
column is the top of each major section.  The second column is the
label to print in the navigation bar

  # Configuration file for the navigation bar
  /index.html             Home
  /new/                   What's New
  /tech/                  Tech Support
  /download/              Download
  /dev/zero               Customer support              
  /dev/null               Complaints

Then, at the top (or bottom) of each HTML page that you want the
navigation bar to appear on, add this comment:

  <!--#NAVBAR-->

Now add C<Apache::NavBar> to your system (Script II.2.2).  This module
parses the configuration file to create a "navigation bar object".  We
then call the navigation bar object's C<to_html()> method in order to
generate the HTML for the navigation bar to display on the current
page (it will be different for each page, depending on what major
section the page is in).

The next section does some checking to avoid transmitting the page
again if it is already cached on the browser. The effective last
modified time for the page is either the modification time of its HTML
source code, or the navbar's configuration file modification date,
whichever is more recent.

The remainder is just looping through the file a section at a time,
searching for the C<E<lt>!--NAVBAR--E<gt>> comment, and substituting
the navigation bar HTML.

  Script II.2.2 Apache::NavBar
  ----------------------------
  
  package Apache::NavBar;
  # file Apache/NavBar.pm
  
  use strict;
  use Apache::Constants qw(:common);
  use Apache::File ();
  
  my %BARS = ();
  my $TABLEATTS   = 'WIDTH="100%" BORDER=1';
  my $TABLECOLOR  = '#C8FFFF';
  my $ACTIVECOLOR = '#FF0000';
  
  sub handler {
      my $r = shift;
  
      my $bar = read_configuration($r)         || return DECLINED;
      $r->content_type eq 'text/html'          || return DECLINED;
      my $fh = Apache::File->new($r->filename) || return DECLINED;
      my $navbar = $bar->to_html($r->uri);
      
      $r->update_mtime($bar->modified);
      $r->set_last_modified;
      my $rc = $r->meets_conditions;
      return $rc unless $rc == OK;
  
      $r->send_http_header;
      return OK if $r->header_only;
  
      local $/ = "";
      while (<$fh>) {
         s:<!--NAVBAR-->:$navbar:oi;
      } continue { 
         $r->print($_); 
      }
  
      return OK;
  }
  
  # read the navigation bar configuration file and return it as a
  # hash.
  sub read_configuration {
      my $r = shift;
      my $conf_file;
      return unless $conf_file = $r->dir_config('NavConf');
      return unless -e ($conf_file = $r->server_root_relative($conf_file));
      my $mod_time = (stat _)[9];
      return $BARS{$conf_file} if $BARS{$conf_file} 
        && $BARS{$conf_file}->modified >= $mod_time;
      return $BARS{$conf_file} = NavBar->new($conf_file);
  }
  
  package NavBar;
  
  # create a new NavBar object
  sub new {
      my ($class,$conf_file) = @_;
      my (@c,%c);
      my $fh = Apache::File->new($conf_file) || return;
      while (<$fh>) {
         chomp;
         s/^\s+//; s/\s+$//;   #fold leading and trailing whitespace
         next if /^#/ || /^$/; # skip comments and empty lines
         next unless my($url, $label) = /^(\S+)\s+(.+)/;
         push @c, $url;     # keep the url in an ordered array
         $c{$url} = $label; # keep its label in a hash
      }
      return bless {'urls' => \@c,
                   'labels' => \%c,
                   'modified' => (stat $conf_file)[9]}, $class;
  }
  
  # return ordered list of all the URIs in the navigation bar
  sub urls  { return @{shift->{'urls'}}; }
  
  # return the label for a particular URI in the navigation bar
  sub label { return $_[0]->{'labels'}->{$_[1]} || $_[1]; }
  
  # return the modification date of the configuration file
  sub modified { return $_[0]->{'modified'}; }
  
  sub to_html {
      my $self = shift;
      my $current_url = shift;
      my @cells;
      for my $url ($self->urls) {
         my $label = $self->label($url);
         my $is_current = $current_url =~ /^$url/;
         my $cell = $is_current ?
             qq(<FONT COLOR="$ACTIVECOLOR">$label</FONT>)
                 : qq(<A HREF="$url">$label</A>);
         push @cells, 
         qq(<TD CLASS="navbar" ALIGN=CENTER BGCOLOR="$TABLECOLOR">$cell</TD>\n);
      }
      return qq(<TABLE $TABLEATTS><TR>@cells</TR></TABLE>\n);
  }
  
  
  1;
  __END__

 <Location />
   SetHandler  perl-script
   PerlHandler Apache::NavBar
   PerlSetVar  NavConf etc/navigation.conf
 </Location>

C<Apache::NavBar> is available on the CPAN, with further improvements.

=head3 On-the-Fly Compression

WU-FTP has a great feature that automatically gzips a file if you
fetch it by name with a I<.gz> extension added.  Why can't Web servers
do that trick?  With Apache and mod_perl, you can.

Script II.2.4 is a content filter that automatically gzips everything
retrieved from a particular directory and adds the "gzip"
C<Content-Encoding> header to it.  Unix versions of Netscape Navigator
will automatically recognize this encoding type and decompress the
file on the fly.  Windows and Mac versions don't.  You'll have to save
to disk and decompress, or install the WinZip plug-in.  Bummer.

The code uses the C<Compress::Zlib> module, and has to do a little
fancy footwork (but not too much) to create the correct gzip header.
You can extend this idea to do on-the-fly encryption, or whatever you
like.

Here's the configuration entry you'll need.  Everything in the
I</compressed> directory will be compressed automagically.

  <Location /compressed>
     SetHandler  perl-script
     PerlHandler Apache::GZip
  </Location>

  Script II.2.3: Apache::GZip
  ---------------------------
  package Apache::GZip;
  #File: Apache::GZip.pm
  
  use strict vars;
  use Apache::Constants ':common';
  use Compress::Zlib;
  use IO::File;
  use constant GZIP_MAGIC => 0x1f8b;
  use constant OS_MAGIC => 0x03;
  
  sub handler {
      my $r = shift;
      my ($fh,$gz);
      my $file = $r->filename;
      return DECLINED unless $fh=IO::File->new($file);
      $r->header_out('Content-Encoding'=>'gzip');
      $r->send_http_header;
      return OK if $r->header_only;
  
      tie *STDOUT,'Apache::GZip',$r;
      print($_) while <$fh>;
      untie *STDOUT;
      return OK;
  }
  
  sub TIEHANDLE {
      my($class,$r) = @_;
      # initialize a deflation stream
      my $d = deflateInit(-WindowBits=>-MAX_WBITS()) || return undef;
  
      # gzip header -- don't ask how I found out
      $r->print(pack("nccVcc",GZIP_MAGIC,Z_DEFLATED,0,time(),0,OS_MAGIC));
  
      return bless { r   => $r,
                     crc =>  crc32(undef),
                     d   => $d,
                     l   =>  0 
                   },$class;
  }
  
  sub PRINT {
      my $self = shift;
      foreach (@_) {
        # deflate the data
        my $data = $self->{d}->deflate($_);
        $self->{r}->print($data);
        # keep track of its length and crc
        $self->{l} += length($_);
        $self->{crc} = crc32($_,$self->{crc});
      }
  }
  
  sub DESTROY {
     my $self = shift;
     
     # flush the output buffers
     my $data = $self->{d}->flush;
     $self->{r}->print($data);
     
     # print the CRC and the total length (uncompressed)
     $self->{r}->print(pack("LL",@{$self}{qw/crc l/}));
  }
   
  1;

For some alternatives that are being maintained, you might want to
look at the C<Apache::Compress> and C<Apache::GzipChain> modules on
CPAN, which can handle the output of any handler in a chain.

By adding a URI translation handler, you can set things up so that a
remote user can append a I<.gz> to the end of any URL and the file we
be delivered in compressed form.  Script II.2.4 shows the translation
handler you need.  It is called during the initial phases of the
request to make any modifications to the URL that it wishes.  In this
case, it removes the I<.gz> ending from the filename and arranges for
C<Apache:GZip> to be called as the content handler.  The
C<lookup_uri()> call is used to exclude anything that has a special
handler already defined (such as CGI scripts), and actual gzip files.
The module replaces the information in the request object with
information about the real file (without the C<.gz>), and arranges for
C<Apache::GZip> to be the content handler for this file.

You just need this one directive to activate handling for all URLs at
your site:

  PerlTransHandler Apache::AutoGZip

  Script II.2.4: Apache::AutoGZip
  -------------------------------
  package Apache::AutoGZip;
  
  use strict 'vars';
  use Apache::Constants qw/:common/;
  
  sub handler {
      my $r = shift;
  
      # don't allow ourselves to be called recursively
      return DECLINED unless $r->is_initial_req;
  
      # don't do anything for files not ending with .gz
      my $uri = $r->uri;
      return DECLINED unless $uri=~/\.gz$/; 
      my $basename = $`;
  
      # don't do anything special if the file actually exists
      return DECLINED if -e $r->lookup_uri($uri)->filename;
  
      # look up information about the file
      my $subr = $r->lookup_uri($basename);
      $r->uri($basename);
      $r->path_info($subr->path_info);
      $r->filename($subr->filename);
  
      # fix the handler to point to Apache::GZip;
      my $handler = $subr->handler;
      unless ($handler) {
        $r->handler('perl-script');
        $r->push_handlers('PerlHandler','Apache::GZip');
      } else {
        $r->handler($handler);
      }
      return OK;
  }
  
  1;

=head2 Access Control

Access control, as opposed to authentication and authorization, is
based on something the user "is" rather than something he "knows".
The "is" is usually something about his browser, such as its IP
address, hostname, or user agent.  Script II.3.1 blocks access to the
Web server for certain User Agents (you might use this to block
impolite robots).

C<Apache::BlockAgent> reads its blocking information from a "bad
agents" file, which contains a series of pattern matches.  Most of the
complexity of the code comes from watching this file and recompiling
it when it changes.  If the file doesn't change, it's is only read
once and its patterns compiled in memory, making this module fast.

Here's an example bad agents file:

   ^teleport pro\/1\.28
   ^nicerspro
   ^mozilla\/3\.0 \(http engine\)
   ^netattache
   ^crescent internet toolpak http ole control v\.1\.0
   ^go-ahead-got-it
   ^wget
   ^devsoft's http component v1\.0
   ^www\.pl
   ^digout4uagent

A configuration entry to activate this blocker looks like this.  In
this case we're blocking access to the entire site.  You could also
block access to a portion of the site, or have different bad agents
files associated with different portions of the document tree.

  <Location />
    PerlAccessHandler Apache::BlockAgent
    PerlSetVar BlockAgentFile /home/www/conf/bad_agents.txt
  </Location>


  Script II.3.1: Apache::BlockAgent
  ---------------------------------
  package Apache::BlockAgent;
  # block browsers that we don't like
  
  use strict 'vars';
  use Apache::Constants ':common';
  use IO::File;
  my %MATCH_CACHE;
  my $DEBUG = 0;
  
  sub handler {
      my $r = shift;
       
      return DECLINED unless my $patfile = $r->dir_config('BlockAgentFile');
      return FORBIDDEN unless my $agent = $r->header_in('User-Agent');
      return SERVER_ERROR unless my $sub = get_match_sub($r,$patfile);
      return OK if $sub->($agent);
      $r->log_reason("Access forbidden to agent $agent",$r->filename);
      return FORBIDDEN;
  }
   
  # This routine creates a pattern matching subroutine from a
  # list of pattern matches stored in a file.
  sub get_match_sub {
      my ($r,$filename) = @_;
      my $mtime = -M $filename;
    
      # try to return the sub from cache
      return $MATCH_CACHE{$filename}->{'sub'} if
          $MATCH_CACHE{$filename} && 
             $MATCH_CACHE{$filename}->{'mod'} <= $mtime;
   
      # if we get here, then we need to create the sub
      return undef unless my $fh = new IO::File($filename);
      chomp(my @pats = <$fh>); # get the patterns into an array
      my $code = "sub { \$_ = shift;\n";
      foreach (@pats) {
          next if /^#/
          $code .= "return undef if /$_/i;\n";
      }
      $code .= "1; }\n";     
      warn $code if $DEBUG;
   
      # create the sub, cache and return it
      my $sub = eval $code;
      unless ($sub) {
          $r->log_error($r->uri,": ",$@);
          return undef;
      }
      @{$MATCH_CACHE{$filename}}{'sub','mod'}=($sub,$modtime);
      return $MATCH_CACHE{$filename}->{'sub'};
  }
   
  1;

=head2 Authentication and Authorization

Thought you were stuck with authentication using text, DBI and DBM
files?  mod_perl opens the authentication/authorization API wide.  The
two phases are authentication, in which the user has to prove who he
or she is (usually by providing a username and password), and
authorization, in which the system decides whether this user has
sufficient privileges to view the requested URL.  A scheme can
incorporate authentication and authorization either together or
singly.

=head3 Authentication with NIS

If you keep Unix system passwords in I</etc/passwd> or distribute them
by NIS (not NIS+) you can authenticate Web users against the system
password database.  (It's not a good idea to do this if the system is
connected to the Internet because passwords travel in the clear, but
it's OK for trusted intranets.)

Script II.4.1 shows how the C<Apache::AuthSystem> module fetches the
user's name and password, compares it to the system password, and
takes appropriate action.  The C<getpwnam()> function operates either
on local files or on the NIS database, depending on how the server
host is configured.  WARNING: the module will fail if you use a shadow
password system, since the Web server doesn't have root privileges.

In order to activate this system, put a configuration directive like
this one in access.conf:

  <Location /protected>
    AuthName Test
    AuthType Basic
    PerlAuthenHandler Apache::AuthSystem
    require valid-user
  </Location>

  Script II.4.1: Apache::AuthSystem
  ---------------------------------
  package Apache::AuthSystem;
  # authenticate users on system password database
  
  use strict;
  use Apache::Constants ':common';
  
  sub handler {
     my $r = shift;
  
     my($res, $sent_pwd) = $r->get_basic_auth_pw;
     return $res if $res != OK;
     
     my $user = $r->connection->user;
     my $reason = "";
  
     my($name,$passwd) = getpwnam($user);
     if (!$name) {
         $reason = "user does not have an account on this system";
     } else {
         $reason = "user did not provide correct password"
           unless $passwd eq crypt($sent_pwd,$passwd);
     }
  
     if($reason) {
        $r->note_basic_auth_failure;
        $r->log_reason($reason,$r->filename);
        return AUTH_REQUIRED;
     }
  
     return OK;
  }
  
  1;

There are modules doing equivalent things on CPAN:
C<Apache::AuthenPasswd> and C<Apache::AuthxPasswd>.

=head3 Anonymous Authentication

Here's a system that authenticates users the way anonymous FTP does.
They have to enter a name like "Anonymous" (configurable) and a
password that looks like a valid e-mail address.  The system rejects
the username and password unless they are formatted correctly.

In a real application, you'd probably want to log the password
somewhere for posterity.  Script II.4.2 shows the code for
C<Apache::AuthAnon>.  To activate it, create a I<httpd.conf> section
like this one:

  <Location /protected>
        AuthName Anonymous
        AuthType Basic
        PerlAuthenHandler Apache::AuthAnon
        require valid-user
  
        PerlSetVar Anonymous anonymous|anybody
  </Location>


  Script II.4.2: Anonymous Authentication
  ---------------------------------------
  package Apache::AuthAnon;
  
  use strict;
  use Apache::Constants ':common';
  
  my $email_pat = '\w+\@\w+\.\w+';
  my $anon_id  = "anonymous";
  
  sub handler {
      my $r = shift;
  
      my($res, $sent_pwd) = $r->get_basic_auth_pw;
      return $res if $res != OK;
  
      my $user = lc $r->connection->user;
      my $reason = "";
  
      my $check_id = $r->dir_config("Anonymous") || $anon_id;
  
      unless($user =~ /^$check_id$/i) {
        $reason = "user did not enter a valid anonymous username";
      }
  
      unless($sent_pwd =~ /$email_pat/o) {
        $reason = "user did not enter an email address password";
      } 
  
      if($reason) {
        $r->note_basic_auth_failure;
        $r->log_reason($reason,$r->filename);
        return AUTH_REQUIRED;
      }
  
      $r->notes(AuthAnonPassword => $sent_pwd);
  
      return OK;
  }
  
  1;


=head3 Gender-Based Authorization

After authenticating, you can authorize.  The most familiar type of
authorization checks a group database to see if the user belongs to
one or more privileged groups.  But authorization can be anything you
dream up.

Script II.4.3 shows how you can authorize users by their gender (or at
least their I<apparent> gender, by checking their names with Jon
Orwant's C<Text::GenderFromName> module.  This must be used in
conjunction with an authentication module, such as one of the standard
Apache modules or a custom one.

This configuration restricts access to users with feminine names,
except for the users "Webmaster" and "Jeff", who are allowed access.

  <Location /ladies_only>
    AuthName "Ladies Only"
    AuthType Basic
    AuthUserFile /home/www/conf/users.passwd
    PerlAuthzHandler  Apache::AuthzGender
    require gender F            # allow females
    require user Webmaster Jeff # allow Webmaster or Jeff
  </Location>

The script uses a custom error response to explain why the user was
denied admittance.  This is better than the standard "Authorization
Failed" message.

  Script II.4.3: Apache::AuthzGender
  ----------------------------------
  package Apache::AuthzGender;
  
  use strict;
  use Text::GenderFromName;
  use Apache::Constants ":common";
  
  my %G=('M'=>"male",'F'=>"female");
  
  sub handler {
      my $r = shift;
     
      return DECLINED unless my $requires = $r->requires;
      my $user = lc($r->connection->user);
      substr($user,0,1)=~tr/a-z/A-Z/;
      my $guessed_gender = uc(gender($user)) || 'M';
  
      my $explanation = <<END;
  <html><head><title>Unauthorized</title></head><body>
  <h1>You Are Not Authorized to Access This Page</h1>
  Access to this page is limited to:
  <ol>
  END
  
      foreach (@$requires) {
        my ($requirement,@rest ) = split(/\s+/,$_->{requirement});
        if (lc $requirement eq 'user') {
             foreach (@rest) { return OK if $user eq $_; }
             $explanation .= "<LI>Users @rest.\n";
        } elsif (lc $requirement eq 'gender') {
             foreach (@rest) { return OK if $guessed_gender eq uc $_; }
             $explanation .= "<LI>People of the @G{@rest} persuasion.\n";
        } elsif (lc $requirement eq 'valid-user') {
             return OK;
        }
      }
  
      $explanation .= "</OL></BODY></HTML>";
     
      $r->custom_response(AUTH_REQUIRED,$explanation);
      $r->note_basic_auth_failure;
      $r->log_reason("user $user: not authorized",$r->filename);
      return AUTH_REQUIRED;
  }

  1;

C<Apache::AuthzGender> is available from the CPAN.

=head2 Proxy Services

mod_perl gives you access to Apache's ability to act as a Web proxy.
You can intervene at any step in the proxy transaction to modify the
outgoing request (for example, stripping off headers in order to
create an anonymizing proxy) or to modify the returned page.

=head3 A Banner Ad Blocker

Script II.5.1 shows the code for a banner-ad blocker written by Doug
MacEachern.  It intercepts all proxy requests, substituting its own
content handler for the default.  The content handler uses the LWP
library to fetch the requested document. If the retrieved document is
an image, and its URL matches the pattern (ads?|advertisement|banner),
then the content of the image is replaced with a dynamically-generated
GIF that reads "Blocked Ad". The generated image is exactly the same
size as the original, preserving the page layout.  Notice how the
outgoing headers from the Apache request object are copied to the LWP
request, and how the incoming LWP response headers are copied back to
Apache.  This makes the transaction nearly transparent to Apache and
to the remote server.

In addition to LWP you'll need C<GD.pm> and C<Image::Size> to run this
module.  To activate it, add the following line to the configuration
file:

  PerlTransHandler Apache::AdBlocker

Then configure your browser to use the server to proxy all its HTTP
requests.  Works like a charm!  With a little more work, and some help
from the C<ImageMagick> module, you could adapt this module to
quiet-down animated GIFs by stripping them of all but the very first
frame.

  Script II.5.1: Apache::AdBlocker
  --------------------------------
  
  package Apache::AdBlocker;
  
  use strict;
  use vars qw(@ISA $VERSION);
  use Apache::Constants qw(:common);
  use GD ();
  use Image::Size qw(imgsize);
  use LWP::UserAgent ();
  
  @ISA = qw(LWP::UserAgent);
  $VERSION = '1.00';
  
  my $UA = __PACKAGE__->new;
  $UA->agent(join "/", __PACKAGE__, $VERSION);
  
  my $Ad = join "|", qw{ads? advertisement banner};
  
  sub handler {
      my($r) = @_;
      return DECLINED unless $r->proxyreq;
      $r->handler("perl-script"); #ok, let's do it
      $r->push_handlers(PerlHandler => \&proxy_handler);
      return OK;
  }
  
  sub proxy_handler {
      my($r) = @_;
   
      my $request = HTTP::Request->new($r->method, $r->uri);
      
      $r->headers_in->do(sub { 
         $request->header(@_); 
      });
  
      # copy POST data, if any
      if($r->method eq 'POST') {
         my $len = $r->header_in('Content-length');
         my $buf;
         $r->read($buf, $len);
         $request->content($buf);
         $request->content_type($r->content_type);
      }
   
      my $response = $UA->request($request);
      $r->content_type($response->header('Content-type'));
  
      #feed response back into our request_rec*
      $r->status($response->code);
      $r->status_line(join " ", $response->code, $response->message);
      $response->scan(sub {
         $r->header_out(@_);
      });
  
      if ($r->header_only) {
         $r->send_http_header();
         return OK;
      }
  
      my $content = \$response->content;
      if($r->content_type =~ /^image/ and $r->uri =~ /\b($Ad)\b/i) {
         block_ad($content);
         $r->content_type("image/gif");
      }
   
      $r->content_type('text/html') unless $$content;
      $r->send_http_header;
      $r->print($$content || $response->error_as_HTML);
  
      return OK;
  }
  
  sub block_ad {
      my $data = shift;
      my($x, $y) = imgsize($data);
   
      my $im = GD::Image->new($x,$y);
   
      my $white = $im->colorAllocate(255,255,255);
      my $black = $im->colorAllocate(0,0,0);       
      my $red = $im->colorAllocate(255,0,0);      
  
      $im->transparent($white);
      $im->string(GD::gdLargeFont(),5,5,"Blocked Ad",$red);
      $im->rectangle(0,0,$x-1,$y-1,$black);
  
      $$data = $im->gif;
  }
  
  1;

Another way of doing this module would be to scan all proxied HTML
files for C<E<lt>imgE<gt>> tags containing one of the verboten URLs,
then replacing the C<src> attribute with a transparent GIF of our own.
However, unless the C<E<lt>imgE<gt>> tag contained C<width> and
C<height> attributes, we wouldn't be able to return a GIF of the
correct size -- unless we were to go hunting for the GIF with LWP, in
which case we might as well do it this way.

=head2 Customized Logging

After Apache handles a transaction, it passes all the information
about the transaction to the log handler.  The default log handler
writes out lines to the log file.  With mod_perl, you can install your
own log handler to do customized logging.

=head3 Send E-Mail When a Particular Page Gets Hit

Script II.6.1 installs a log handler which watches over a page or set
of pages.  When someone fetches a watched page, the log handler sends
off an e-mail to notify someone (probably the owner of the page) that
the page has been read.

To activate the module, just attach a C<PerlLogHandler> to the
C<E<lt>LocationE<gt>> or C<E<lt>FilesE<gt>> you wish to watch.  For
example:

   <Location /~lstein>
      PerlLogHandler Apache::LogMail
      PerlSetVar mailto lstein@cshl.org
   </Location>

The "mailto" directive specifies the name of the recipient(s) to
notify.


  Script II.6.1: Apache::LogMail
  ------------------------------
  package Apache::LogMail;
  use Apache::Constants ':common';
  
  sub handler {
      my $r = shift;
      my $mailto = $r->dir_config('mailto');
      return DECLINED unless $mailto
      my $request = $r->the_request;
      my $uri = $r->uri;
      my $agent = $r->header_in("User-agent");
      my $bytes = $r->bytes_sent;
      my $remote = $r->get_remote_host;
      my $status = $r->status_line;
      my $date = localtime;
      unless (open (MAIL,"|/usr/lib/sendmail -oi -t")) {
         $r->log_error("Couldn't open mail: $!");
         return DECLINED;
      }
      print MAIL <<END;
  To: $mailto
  From: Mod Perl <webmaster>
  Subject: Somebody looked at $uri
  
  At $date, a user at $remote looked at
  $uri using the $agent browser.  
  
  The request was $request, 
  which resulted returned a code of $status.
  
  $bytes bytes were transferred.
  END
      close MAIL;
      return OK;
  }
  1;

=head3 Writing Log Information Into a Relational Database

Coming full circle, Script II.6.2 shows a module that writes log
information into a DBI database.  The idea is similar to Script I.1.9,
but there's now no need to open a pipe to an external process.  It's
also a little more efficient, because the log data fields can be
recovered directly from the Apache request object, rather than parsed
out of a line of text.  Another improvement is that we can set up the
Apache configuration files so that only accesses to certain
directories are logged in this way.

To activate, add something like this to your configuration file:
 
  PerlLogHandler Apache::LogDBI

Or, to restrict special logging to accesses of files in below the URL
"/lincoln_logs" add this:

  <Location /lincoln_logs>
    PerlLogHandler Apache::LogDBI  
  </Location>

  Script II.6.2: Apache::LogDBI
  -----------------------------
  package Apache::LogDBI;
  use Apache::Constants ':common';
  
  use strict 'vars';
  use vars qw($DB $STH);
  use DBI;
  use POSIX 'strftime';
  
  use constant DSN       => 'dbi:mysql:www';
  use constant DB_TABLE  => 'access_log';
  use constant DB_USER   => 'nobody';
  use constant DB_PASSWD => '';
  
  $DB = DBI->connect(DSN,DB_USER,DB_PASSWD) || die DBI->errstr;
  $STH = $DB->prepare("INSERT INTO ${\DB_TABLE} VALUES(?,?,?,?,?,?,?,?,?)") 
       || die $DB->errstr;
  
  sub handler {
      my $r = shift;
      my $date    = strftime('%Y-%m-%d %H:%M:%S',localtime);
      my $host    = $r->get_remote_host;
      my $method  = $r->method;
      my $url     = $r->uri;
      my $user    = $r->connection->user;
      my $referer = $r->header_in('Referer');
      my $browser = $r->header_in("User-agent");
      my $status  = $r->status;
      my $bytes   = $r->bytes_sent;
      $STH->execute($date,$host,$method,$url,$user,
                    $browser,$referer,$status,$bytes);
      return OK;
  }
  
  1;

There are other alternatives which are more actively maintained
available from the CPAN: C<Apache::DBILogger> and
C<Apache::DBILogConfig>.

=head1 Conclusion

These tricks illustrate the true power of mod_perl; not only were Perl
and Apache good friends from the start, thanks to Perl's excellent
text-handling capacity, but when mod_perl is used, your complete
access to the Apache API gives you unprecendented power in dynamic web
serving.

To find more tips and tricks, look for modules on the CPAN, look
through the mod_perl L<documentation|docs::index>, and also in the
following books by Lincoln Stein:

=over 4

=item "How to Set Up and Maintain a Web Site"

General introduction to Web site care and feeding, with an emphasis on
Apache.  Addison-Wesley 1997.

Companion Web site at http://www.genome.wi.mit.edu/WWW/

=item "Web Security, a Step-by-Step Reference Guide"

How to keep your Web site free from thieves, vandals, hooligans and
other yahoos.  Addison-Wesley 1998.

Companion Web site at http://www.w3.org/Security/Faq/

=item "The Official Guide to Programming with CGI.pm"

Everything I know about CGI.pm (and some things I don't!).  John Wiley
& Sons, 1998.

Companion Web site at http://www.wiley.com/compbooks/stein/

=item "Writing Apache Modules in Perl and C"

Co-authored with Doug MacEachern. O'Reilly & Associates.

Companion Web site at http://www.modperl.com/

=item WebTechniques Columns

I write a monthly column for WebTechniques magazine.  You can find
back-issues and reprints at http://www.web-techniques.com/

=item The Perl Journal Columns

I write a quarterly column for TPJ. Source code listings are available
at http://www.tpj.com/

=back


=head1 Maintainers

Maintainer is the person(s) you should contact with updates,
corrections and patches.

Per Einar Ellefsen E<lt>per.einar (at) skynet.beE<gt>

=head1 Authors

=over

=item * Lincoln Stein E<lt>lstein (at) cshl.orgE<gt>

=back

Only the major authors are listed above. For contributors see the
Changes file.

=cut
